{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Función para instalar OpenJDK 8 si no está instalado previamente.\n",
        "def install_java():\n",
        "    # Verifica si la ruta del JDK existe.\n",
        "    if not os.path.exists('/usr/lib/jvm/java-8-openjdk-amd64'):\n",
        "        print(\"Instalando OpenJDK 8...\")\n",
        "        # Comando para instalar Java en modo silencioso.\n",
        "        !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "        print(\"OpenJDK 8 instalado correctamente.\")\n",
        "    else:\n",
        "        print(\"OpenJDK 8 ya está instalado.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para descargar Apache Spark solo si no está ya descargado.\n",
        "def download_spark():\n",
        "    # URL y nombre del archivo comprimido de Spark.\n",
        "    spark_url = \"https://archive.apache.org/dist/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\"\n",
        "    spark_tar = \"spark-3.4.3-bin-hadoop3.tgz\"\n",
        "\n",
        "    # Verifica si el archivo comprimido ya existe.\n",
        "    if not os.path.exists(spark_tar):\n",
        "        print(\"Descargando Apache Spark...\")\n",
        "        # Comando para descargar el archivo desde la URL.\n",
        "        !wget -q $spark_url\n",
        "        print(\"Descarga completa.\")\n",
        "    else:\n",
        "        print(\"El archivo de Apache Spark ya está descargado.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para descomprimir el archivo de Apache Spark solo si no está descomprimido.\n",
        "def extract_spark():\n",
        "    # Nombre de la carpeta descomprimida.\n",
        "    spark_dir = \"spark-3.4.3-bin-hadoop3\"\n",
        "    spark_tar = \"spark-3.4.3-bin-hadoop3.tgz\"\n",
        "\n",
        "    # Verifica si la carpeta descomprimida ya existe.\n",
        "    if not os.path.exists(spark_dir):\n",
        "        print(\"Descomprimiendo Apache Spark...\")\n",
        "        # Comando para descomprimir el archivo.\n",
        "        !tar xf $spark_tar\n",
        "        print(\"Apache Spark descomprimido.\")\n",
        "    else:\n",
        "        print(\"La carpeta de Apache Spark ya existe.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para configurar las variables de entorno necesarias para Spark.\n",
        "def set_environment_variables():\n",
        "    # Establece la ruta de JAVA_HOME y SPARK_HOME.\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\"\n",
        "    print(\"Variables de entorno configuradas.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para verificar si un paquete de Python ya está instalado.\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def is_package_installed(package_name):\n",
        "    try:\n",
        "        # Ejecuta el comando `pip show` para verificar si el paquete está instalado.\n",
        "        subprocess.check_call(\n",
        "            [sys.executable, \"-m\", \"pip\", \"show\", package_name],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.DEVNULL\n",
        "        )\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "# Función para instalar librerías de Python necesarias (findspark y pyspark).\n",
        "def install_python_libraries():\n",
        "    # Verifica e instala findspark.\n",
        "    if not is_package_installed(\"findspark\"):\n",
        "        print(\"Instalando findspark...\")\n",
        "        !pip install -q findspark\n",
        "    else:\n",
        "        print(\"findspark ya está instalado.\")\n",
        "\n",
        "    # Verifica e instala pyspark.\n",
        "    if not is_package_installed(\"pyspark\"):\n",
        "        print(\"Instalando pyspark...\")\n",
        "        !pip install -q pyspark\n",
        "    else:\n",
        "        print(\"pyspark ya está instalado.\")\n",
        "\n",
        "\n",
        "\n",
        "install_java()                # Instala OpenJDK 8\n",
        "download_spark()              # Descarga Apache Spark\n",
        "extract_spark()               # Descomprime Apache Spark\n",
        "set_environment_variables()   # Configura las variables de entorno\n",
        "install_python_libraries()    # Instala las librerías de Python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXwui0oU2E-x",
        "outputId": "297af0e2-3278-4b01-c968-658bd95b50f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenJDK 8 ya está instalado.\n",
            "El archivo de Apache Spark ya está descargado.\n",
            "La carpeta de Apache Spark ya existe.\n",
            "Variables de entorno configuradas.\n",
            "findspark ya está instalado.\n",
            "pyspark ya está instalado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **Crear una sesion en Spark**"
      ],
      "metadata": {
        "id": "t5i-OfFAdt6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la biblioteca findspark, que ayuda a configurar PySpark correctamente en el entorno de ejecución\n",
        "import findspark\n",
        "\n",
        "# Inicializar findspark para establecer las variables necesarias de PySpark\n",
        "findspark.init()\n",
        "\n",
        "# Importar SparkSession desde la biblioteca pyspark.sql\n",
        "# SparkSession es la entrada principal para trabajar con DataFrames en PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear o obtener una SparkSession\n",
        "# Esto inicializa un entorno de Spark si no existe uno, o reutiliza uno existente\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Obtener el contexto de Spark (SparkContext) desde la SparkSession\n",
        "# SparkContext es la entrada principal para trabajar con RDDs en PySpark\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "eztf5DxddjDm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "RP6dgxU9ugJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PARTICIONADO**\n",
        "\n",
        "En Spark, una **partición** es la unidad más pequeña de datos que se puede procesar de forma paralela.\n",
        "\n",
        "- Es un **fragmento de un conjunto de datos** (como un DataFrame o RDD) dividido en partes para distribuir el procesamiento entre los nodos del clúster.\n",
        "- Cada partición se procesa de manera independiente, lo que permite a Spark manejar grandes volúmenes de datos de forma eficiente.\n",
        "\n",
        "**Ejemplo:** Si tienes un archivo grande con 1 millón de filas y lo divides en 10 particiones, cada partición manejará 100,000 filas. Esto permite que Spark procese estas particiones en paralelo en diferentes nodos.\n",
        "\n",
        "\n",
        "*Material de estudios:*\n",
        "\n",
        "*   [Particionamiento de Datos en Apache Spark I](https://medium.com/iwannabedatadriven/particionamiento-de-datos-en-spark-df4eb0933e74)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VqgmhNTeulco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **PARTICIONADORES EN SPARK**\n",
        "**Diferencias entre HashPartitioner y RangePartitioner en Apache Spark**\n",
        "\n",
        "\n",
        "Los particionadores en Spark determinan cómo se distribuyen los datos entre las particiones. Son clave para optimizar el procesamiento distribuido, especialmente en transformaciones como `groupByKey` o `reduceByKey`.\n",
        "\n",
        "### 1. **HashPartitioner**\n",
        "- **Cómo funciona:**\n",
        "  - Divide los datos en particiones basándose en el **hash** de la clave.\n",
        "  - El número de particiones se define explícitamente (por ejemplo, `numPartitions=4`).\n",
        "  - Las claves con el mismo hash se colocan en la misma partición.\n",
        "\n",
        "- **Uso común:**\n",
        "  - Ideal para datos con claves distribuidas uniformemente.\n",
        "  - Se usa automáticamente en transformaciones como `reduceByKey`.\n",
        "\n",
        "- **Ejemplo:**\n",
        "  Si tienes las claves `A`, `B`, `C` y `D`, el HashPartitioner calcula el hash de cada clave y las asigna a particiones, como:\n",
        "  ```\n",
        "  Partición 0: A, C\n",
        "  Partición 1: B, D\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **RangePartitioner**\n",
        "- **Cómo funciona:**\n",
        "  - Divide los datos en particiones en función de un **rango ordenado** de las claves.\n",
        "  - Requiere que las claves tengan un orden definido (por ejemplo, valores numéricos o cadenas ordenables).\n",
        "  - Se basa en muestreos de las claves para determinar los rangos de cada partición.\n",
        "\n",
        "- **Uso común:**\n",
        "  - Ideal para datos ordenados o cuando necesitas mantener un orden entre las particiones.\n",
        "  - Se usa en aplicaciones que requieren búsquedas rápidas en rangos.\n",
        "\n",
        "- **Ejemplo:**\n",
        "  Si tienes claves numéricas `[1, 3, 5, 7, 9]` y 2 particiones:\n",
        "  ```\n",
        "  Partición 0: [1, 3, 5]\n",
        "  Partición 1: [7, 9]\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Diferencias clave:**\n",
        "| Aspecto               | HashPartitioner                 | RangePartitioner                |\n",
        "|-----------------------|---------------------------------|---------------------------------|\n",
        "| **Distribución**      | Basada en el hash de la clave.  | Basada en rangos ordenados.     |\n",
        "| **Orden de las claves** | No garantiza orden.            | Garantiza orden parcial.        |\n",
        "| **Datos ideales**      | Claves uniformemente distribuidas. | Claves ordenadas o con rangos definidos. |\n",
        "\n",
        "Ambos particionadores mejoran la eficiencia, pero elegir uno depende de los datos y la tarea."
      ],
      "metadata": {
        "id": "DWX-9uLQfVHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejemplo práctico de HashPartitioner en Apache Spark: creación, distribución y análisis de particiones**"
      ],
      "metadata": {
        "id": "X71nXnmTieAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD más grande con pares clave-valor\n",
        "# Cada elemento del RDD es un par (clave, valor), donde la clave es un nombre de fruta y el valor es un número asociado.\n",
        "rdd = sc.parallelize([(\"manzana\", 10), (\"naranja\", 20), (\"pera\", 30),\n",
        "                      (\"uva\", 40), (\"mango\", 50), (\"piña\", 60),\n",
        "                      (\"fresa\", 70), (\"kiwi\", 80), (\"cereza\", 90),\n",
        "                      (\"sandía\", 100), (\"melon\", 110), (\"limón\", 120)])\n",
        "\n",
        "# Número de particiones\n",
        "# Este valor define en cuántas partes queremos dividir los datos del RDD.\n",
        "num_particiones = 3\n",
        "\n",
        "# Paso 1: Aplicar HashPartitioner a nuestro RDD de clave-valor\n",
        "# `partitionBy` utiliza un particionador basado en hash para asignar cada clave a una partición.\n",
        "# Las claves con el mismo hash (módulo del número de particiones) irán a la misma partición.\n",
        "partitioned_rdd = rdd.partitionBy(num_particiones)\n",
        "\n",
        "# Paso 2: Verificar la distribución de los datos en las particiones\n",
        "# Aquí usamos `mapPartitionsWithIndex` para inspeccionar cómo los datos están distribuidos en cada partición.\n",
        "# La función lambda recibe el índice de la partición (idx) y los datos dentro de esa partición (it).\n",
        "partitioned_data = partitioned_rdd.mapPartitionsWithIndex(\n",
        "    lambda idx, it: [(idx, list(it))],  # Convertimos los datos a una lista junto con el índice de partición.\n",
        "    preservesPartitioning=True         # Indicamos que mantenemos la partición original del RDD.\n",
        ")\n",
        "\n",
        "# Mostrar los datos repartidos por partición\n",
        "# Recorremos los resultados para imprimir qué datos se encuentran en cada partición.\n",
        "print(\"Distribución de datos por partición:\")\n",
        "for idx, items in partitioned_data.collect():  # `collect` reúne todos los datos distribuidos para mostrarlos.\n",
        "    print(f\"Partición {idx}: {items}\")\n",
        "\n",
        "# Paso 3: Verificar el hash de cada clave y la partición asignada\n",
        "# Para cada clave en el RDD original, calculamos su hash y determinamos en qué partición será asignada.\n",
        "# La partición se calcula como: hash(clave) % num_particiones.\n",
        "print(\"\\nHash y partición asignada para cada clave:\")\n",
        "hashes = rdd.map(lambda x: (x[0], hash(x[0]), hash(x[0]) % num_particiones))\n",
        "\n",
        "# Imprimimos los valores de hash y la partición calculada para cada clave\n",
        "for clave, h, partition in hashes.collect():  # `collect` reúne los valores hash calculados para imprimirlos.\n",
        "    print(f\"Clave: {clave}, Hash: {h}, Partición: {partition}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp0m4JEfnlcI",
        "outputId": "e87e13a3-a776-4573-b1eb-9c985cc771b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribución de datos por partición:\n",
            "Partición 0: [('manzana', 10), ('naranja', 20), ('uva', 40), ('mango', 50), ('cereza', 90)]\n",
            "Partición 1: [('pera', 30), ('fresa', 70), ('kiwi', 80)]\n",
            "Partición 2: [('piña', 60), ('sandía', 100), ('melon', 110), ('limón', 120)]\n",
            "\n",
            "Hash y partición asignada para cada clave:\n",
            "Clave: manzana, Hash: -6212662399204218333, Partición: 0\n",
            "Clave: naranja, Hash: 1245446572899628356, Partición: 0\n",
            "Clave: pera, Hash: -3592559662256592542, Partición: 1\n",
            "Clave: uva, Hash: 1289978012078186706, Partición: 0\n",
            "Clave: mango, Hash: 7509992797015814037, Partición: 0\n",
            "Clave: piña, Hash: -5831872772280901141, Partición: 2\n",
            "Clave: fresa, Hash: 2182514332187640556, Partición: 1\n",
            "Clave: kiwi, Hash: -8292140047809600230, Partición: 1\n",
            "Clave: cereza, Hash: 614178665651577378, Partición: 0\n",
            "Clave: sandía, Hash: -5422801810092505891, Partición: 2\n",
            "Clave: melon, Hash: 5812577297748102197, Partición: 2\n",
            "Clave: limón, Hash: 3012300008299366733, Partición: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "j82j0HMsxqGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Particionamiento de Datos en Spark con RangePartitioner: Implementación y Validación**"
      ],
      "metadata": {
        "id": "p-UTUamzoVXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD más grande con pares clave-valor\n",
        "# Cada par contiene una clave (fruta) y un valor (número asociado).\n",
        "rdd = sc.parallelize([\n",
        "    (\"manzana\", 10), (\"naranja\", 20), (\"pera\", 30),\n",
        "    (\"uva\", 40), (\"mango\", 50), (\"piña\", 60),\n",
        "    (\"fresa\", 70), (\"kiwi\", 80), (\"cereza\", 90),\n",
        "    (\"sandía\", 100), (\"melon\", 110), (\"limón\", 120)\n",
        "])\n",
        "\n",
        "# Paso 1: Determinar los rangos manualmente\n",
        "# 1. Ordenamos las claves del RDD en orden alfabético.\n",
        "sorted_keys = sorted(rdd.map(lambda x: x[0]).collect())  # Se extraen las claves y se ordenan.\n",
        "\n",
        "# 2. Dividimos las claves ordenadas en rangos según el número de particiones.\n",
        "num_particiones = 3  # Especificamos que queremos dividir los datos en 3 particiones.\n",
        "# Calculamos los puntos de corte para los rangos en función del número de particiones.\n",
        "rangos = [sorted_keys[i * len(sorted_keys) // num_particiones] for i in range(1, num_particiones)]\n",
        "# `rangos` contiene los límites superiores de cada partición excepto la última.\n",
        "print(f\"Rangos para las particiones: {rangos}\")  # Mostramos los rangos calculados.\n",
        "\n",
        "# Paso 2: Función de partición personalizada\n",
        "def range_partitioner(clave):\n",
        "    \"\"\"\n",
        "    Función que asigna una clave a una partición basándose en los rangos predefinidos.\n",
        "    Parámetros:\n",
        "        clave (str): La clave a particionar (nombre de la fruta).\n",
        "    Retorna:\n",
        "        int: El índice de la partición correspondiente.\n",
        "    \"\"\"\n",
        "    for i, limite in enumerate(rangos):  # Iteramos por los límites de los rangos.\n",
        "        if clave < limite:  # Si la clave es menor que un límite, asignamos esa partición.\n",
        "            return i\n",
        "    return len(rangos)  # Si no es menor que ningún límite, se asigna a la última partición.\n",
        "\n",
        "# Paso 3: Aplicar la partición al RDD\n",
        "# Usamos `partitionBy` con el particionador personalizado para dividir los datos según los rangos.\n",
        "partitioned_rdd = rdd.partitionBy(num_particiones, lambda k: range_partitioner(k))\n",
        "\n",
        "# Paso 4: Inspeccionar las particiones\n",
        "# Usamos `mapPartitionsWithIndex` para inspeccionar los datos de cada partición.\n",
        "# Para cada partición, generamos una lista que incluye el índice de la partición y los datos de esta.\n",
        "partitioned_data = partitioned_rdd.mapPartitionsWithIndex(\n",
        "    lambda idx, it: [(idx, list(it))],  # `idx` es el índice de la partición, `it` son los datos en ella.\n",
        "    preservesPartitioning=True  # Indicamos que mantenemos la partición original.\n",
        ")\n",
        "\n",
        "# Mostramos los datos distribuidos en cada partición.\n",
        "print(\"\\nDistribución de datos por partición (simulación de RangePartitioner):\")\n",
        "for idx, items in partitioned_data.collect():  # Recolectamos los datos de las particiones y los imprimimos.\n",
        "    print(f\"Partición {idx}: {items}\")\n",
        "\n",
        "# Paso 5: Validar cómo las claves fueron organizadas por rango\n",
        "print(\"\\nValidación del orden de las claves dentro de cada partición:\")\n",
        "for idx, items in partitioned_data.collect():\n",
        "    # Extraemos solo las claves de los datos en cada partición.\n",
        "    claves = [item[0] for item in items]\n",
        "    # Mostramos las claves en cada partición para verificar su organización.\n",
        "    print(f\"Partición {idx}: Claves ordenadas: {claves}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrmSXz_doV_n",
        "outputId": "c7015dda-36c2-4f8d-b90a-ba1d9182657f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rangos para las particiones: ['mango', 'pera']\n",
            "\n",
            "Distribución de datos por partición (simulación de RangePartitioner):\n",
            "Partición 0: [('fresa', 70), ('kiwi', 80), ('cereza', 90), ('limón', 120)]\n",
            "Partición 1: [('manzana', 10), ('naranja', 20), ('mango', 50), ('melon', 110)]\n",
            "Partición 2: [('pera', 30), ('uva', 40), ('piña', 60), ('sandía', 100)]\n",
            "\n",
            "Validación del orden de las claves dentro de cada partición:\n",
            "Partición 0: Claves ordenadas: ['fresa', 'kiwi', 'cereza', 'limón']\n",
            "Partición 1: Claves ordenadas: ['manzana', 'naranja', 'mango', 'melon']\n",
            "Partición 2: Claves ordenadas: ['pera', 'uva', 'piña', 'sandía']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicación de los resultados\n",
        "\n",
        "**1. Rangos para las particiones: `['mango', 'pera']`**\n",
        "- Los rangos definidos (`'mango'` y `'pera'`) dividen las claves en tres segmentos:\n",
        "  - **Partición 0:** Claves menores a `'mango'`.\n",
        "  - **Partición 1:** Claves mayores o iguales a `'mango'` pero menores a `'pera'`.\n",
        "  - **Partición 2:** Claves mayores o iguales a `'pera'`.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Distribución de datos por partición**\n",
        "\n",
        "- **Partición 0:**\n",
        "  Contiene las claves que son menores a `'mango'` alfabéticamente:\n",
        "  - `'fresa'`, `'kiwi'`, `'cereza'`, y `'limón'`.\n",
        "\n",
        "- **Partición 1:**\n",
        "  Contiene las claves que están en el rango de `'mango'` a `'pera'` (excluyendo `'pera'`):\n",
        "  - `'manzana'`, `'naranja'`, `'mango'`, y `'melon'`.\n",
        "\n",
        "- **Partición 2:**\n",
        "  Contiene las claves mayores o iguales a `'pera'`:\n",
        "  - `'pera'`, `'uva'`, `'piña'`, y `'sandía'`.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Validación del orden de las claves dentro de cada partición**\n",
        "- Las claves en cada partición están ordenadas de acuerdo a su orden de aparición en el RDD original. Esto se debe a que **`partitionBy` no reordena los datos dentro de cada partición**, sino que simplemente los asigna a las particiones según la lógica del particionador.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Por qué las claves se distribuyen así**\n",
        "- El particionador utiliza las reglas de comparación de cadenas para determinar la posición alfabética de las claves. Las claves se comparan carácter por carácter siguiendo el estándar Unicode:\n",
        "  - `'a'` < `'b'` < ... < `'z'`.\n",
        "  - Por ejemplo, `'fresa'` es menor que `'mango'` porque `'f'` viene antes de `'m'`.\n",
        "\n",
        "Dentro de cada partición, el orden de las claves refleja el orden en que aparecen originalmente en el RDD. Como no se aplicó una operación explícita para ordenar las claves dentro de las particiones, permanecen en su orden original.\n",
        "\n",
        "---\n",
        "\n",
        "### Resumen\n",
        "- Los rangos definidos (`['mango', 'pera']`) dividen las claves en tres grupos basados en su posición alfabética.\n",
        "- La distribución por particiones depende únicamente de estos rangos, y no hay un reordenamiento interno dentro de las particiones.\n",
        "- Si se necesita que las claves dentro de cada partición estén ordenadas alfabéticamente, sería necesario aplicar una transformación adicional como `sortByKey`."
      ],
      "metadata": {
        "id": "KxuKQXiCpemQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "TNt1qnfDu5UY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MEZCLA DE DATOS (shuffling)**"
      ],
      "metadata": {
        "id": "HdT9lXePuusw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mezcla de Datos (Shuffling) en Spark**\n",
        "\n",
        "La **mezcla de datos** (o *shuffling*) es un proceso crucial en sistemas distribuidos como Apache Spark, donde los datos se reorganizan y se redistribuyen a través de varias particiones o nodos del clúster. Este proceso suele ocurrir durante operaciones que implican una reordenación significativa de los datos, como `groupByKey()`, `reduceByKey()`, `join()`, `distinct()`, entre otras.\n",
        "\n",
        "### ¿Por qué ocurre el shuffling?\n",
        "\n",
        "En Spark, los datos están distribuidos en diferentes particiones dentro de un clúster. Sin embargo, muchas veces es necesario que los datos relacionados (por ejemplo, las claves de un `groupByKey`) estén en la misma partición para poder realizar el procesamiento adecuado. El *shuffling* es la forma en que Spark redistribuye los datos entre las particiones para que la operación pueda realizarse correctamente.\n",
        "\n",
        "### Proceso de Shuffling\n",
        "\n",
        "El proceso de *shuffling* implica tres pasos principales:\n",
        "\n",
        "1. **Lectura de Datos**: Cada nodo o partición lee los datos que le corresponden y prepara el conjunto de datos que deben enviarse a otros nodos.\n",
        "2. **Reorganización**: Los datos se envían a las particiones correspondientes según la clave o el criterio de la operación. En este paso, las claves que deben ser procesadas juntas (por ejemplo, las mismas claves en una operación de `groupByKey`) son movidas a las mismas particiones.\n",
        "3. **Reescritura en las Particiones Destino**: Finalmente, los datos reorganizados se escriben de nuevo en las particiones, lo que permite que la operación final se realice en los datos correctos.\n",
        "\n",
        "### Impacto en el Rendimiento\n",
        "\n",
        "El *shuffling* es una operación costosa en términos de tiempo y recursos. Esto se debe a varias razones:\n",
        "\n",
        "- **Transferencia de Datos**: Implica el movimiento de grandes cantidades de datos entre nodos, lo que puede ser lento dependiendo de la infraestructura de red.\n",
        "- **Almacenamiento Intermedio**: Los datos deben almacenarse temporalmente en el disco antes de ser procesados, lo que puede generar latencia si el disco es lento.\n",
        "- **Ordenación y Cálculos Adicionales**: El proceso de *shuffling* también implica ordenar los datos o aplicar otras transformaciones, lo cual aumenta la complejidad computacional.\n",
        "\n",
        "### Minimizar el Shuffling\n",
        "\n",
        "Aunque el *shuffling* es inevitable en algunas operaciones, existen técnicas para minimizar su impacto:\n",
        "\n",
        "- **Usar operaciones más eficientes**: Por ejemplo, `reduceByKey()` en lugar de `groupByKey()`, ya que reduce los datos antes de que se distribuyan entre particiones.\n",
        "- **Ajustar el número de particiones**: Controlar el número de particiones para evitar un número excesivo de movimientos de datos.\n",
        "- **Evitar operaciones innecesarias**: Reducir el uso de operaciones que desencadenan *shuffling* a menos que sea absolutamente necesario.\n",
        "\n",
        "En resumen, el *shuffling* es un proceso fundamental en Spark para distribuir y reorganizar los datos en función de las operaciones, pero es una de las operaciones más costosas en términos de rendimiento. Por lo tanto, es importante entender cómo y cuándo ocurre para optimizar el rendimiento de las aplicaciones distribuidas."
      ],
      "metadata": {
        "id": "MoHVWtyP889O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 1: `groupByKey()`\n",
        "\n",
        "En este ejemplo, vamos a usar `groupByKey()`, que es una operación que dispara un *shuffling* para agrupar los valores por su clave."
      ],
      "metadata": {
        "id": "2HJ5wUY1-jT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con pares clave-valor\n",
        "rdd = sc.parallelize([(\"manzana\", 10), (\"naranja\", 20), (\"manzana\", 15), (\"uva\", 30), (\"naranja\", 25)])\n",
        "\n",
        "# Realizar un groupByKey() que agrupa los valores por clave\n",
        "grouped_rdd = rdd.groupByKey()\n",
        "\n",
        "# Ver los resultados\n",
        "print(\"Resultado de groupByKey:\")\n",
        "for key, values in grouped_rdd.collect():\n",
        "    print(f\"Clave: {key}, Valores: {list(values)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1v_WiYk-q9-",
        "outputId": "7d914b2e-8a12-4c0b-edd4-0dd99a8be06b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado de groupByKey:\n",
            "Clave: naranja, Valores: [20, 25]\n",
            "Clave: uva, Valores: [30]\n",
            "Clave: manzana, Valores: [10, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 2: `reduceByKey()`\n",
        "\n",
        "`reduceByKey()` es una operación eficiente que realiza un *shuffling* pero minimizando la cantidad de datos que se tienen que mover entre particiones, ya que reduce los valores por clave antes de realizar la mezcla de datos."
      ],
      "metadata": {
        "id": "UYQvz4Wl-v0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con pares clave-valor\n",
        "rdd = sc.parallelize([(\"manzana\", 10), (\"naranja\", 20), (\"manzana\", 15), (\"uva\", 30), (\"naranja\", 25)])\n",
        "\n",
        "# Realizar un reduceByKey() para sumar los valores por clave\n",
        "reduced_rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Ver los resultados\n",
        "print(\"Resultado de reduceByKey:\")\n",
        "for key, value in reduced_rdd.collect():\n",
        "    print(f\"Clave: {key}, Suma: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfcsgP-J-y02",
        "outputId": "490a45e0-7843-4e75-9542-1051d617f688"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado de reduceByKey:\n",
            "Clave: naranja, Suma: 45\n",
            "Clave: uva, Suma: 30\n",
            "Clave: manzana, Suma: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 3: `join()`\n",
        "\n",
        "El uso de `join()` entre dos RDDs también genera un *shuffling*, ya que los datos con claves coincidentes de ambos RDDs deben ser redistribuidos y reorganizados en las particiones."
      ],
      "metadata": {
        "id": "Frm2Zf6z-2_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear dos RDDs con pares clave-valor\n",
        "rdd1 = sc.parallelize([(\"manzana\", 10), (\"naranja\", 20), (\"uva\", 30)])\n",
        "rdd2 = sc.parallelize([(\"manzana\", 5), (\"naranja\", 15), (\"uva\", 25)])\n",
        "\n",
        "# Realizar un join entre los dos RDDs\n",
        "joined_rdd = rdd1.join(rdd2)\n",
        "\n",
        "# Ver los resultados\n",
        "print(\"Resultado de join:\")\n",
        "for key, value in joined_rdd.collect():\n",
        "    print(f\"Clave: {key}, Valores: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp6OlY_Z-6IP",
        "outputId": "809f3b58-6432-4c57-b160-e367cdb2b2f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado de join:\n",
            "Clave: naranja, Valores: (20, 15)\n",
            "Clave: uva, Valores: (30, 25)\n",
            "Clave: manzana, Valores: (10, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 4: `distinct()`\n",
        "\n",
        "La operación `distinct()` realiza un *shuffling* para eliminar los elementos duplicados en un RDD."
      ],
      "metadata": {
        "id": "n1UbHM2d-_Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con elementos duplicados\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 4, 5, 5, 6, 7, 8, 8])\n",
        "\n",
        "# Realizar un distinct() para eliminar duplicados\n",
        "distinct_rdd = rdd.distinct()\n",
        "\n",
        "# Ver los resultados\n",
        "print(\"Resultado de distinct:\")\n",
        "for value in distinct_rdd.collect():\n",
        "    print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxRymWW4_Bof",
        "outputId": "351c1600-c167-4dd7-8adb-ed0a4f1757df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado de distinct:\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "1\n",
            "3\n",
            "5\n",
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 5: `sortBy()`\n",
        "\n",
        "La operación `sortBy()` también puede involucrar *shuffling* si los datos no están localmente ordenados en las particiones y Spark necesita redistribuir los datos para ordenar las claves."
      ],
      "metadata": {
        "id": "jxu7el1d_GdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con datos desordenados\n",
        "rdd = sc.parallelize([(\"manzana\", 30), (\"uva\", 10), (\"naranja\", 20), (\"pera\", 25)])\n",
        "\n",
        "# Realizar un sortBy() para ordenar por el valor\n",
        "sorted_rdd = rdd.sortBy(lambda x: x[1])\n",
        "\n",
        "# Ver los resultados\n",
        "print(\"Resultado de sortBy:\")\n",
        "for key, value in sorted_rdd.collect():\n",
        "    print(f\"Clave: {key}, Valor: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2bPaUy0_Hw-",
        "outputId": "6216560a-c792-4812-d7c2-33205a46f3dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado de sortBy:\n",
            "Clave: uva, Valor: 10\n",
            "Clave: naranja, Valor: 20\n",
            "Clave: pera, Valor: 25\n",
            "Clave: manzana, Valor: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 5: `distinct() + sortBy()`\n",
        "\n",
        "Si necesitas que los datos estén ordenados después de eliminar los duplicados, puedes aplicar una operación adicional de ordenación. Aquí te muestro cómo hacerlo:"
      ],
      "metadata": {
        "id": "x9KEw9zWA1Wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con elementos duplicados\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 4, 5, 5, 6, 7, 8, 8])\n",
        "\n",
        "# Realizar un distinct() para eliminar duplicados\n",
        "distinct_rdd = rdd.distinct()\n",
        "\n",
        "# Ordenar los resultados\n",
        "sorted_rdd = distinct_rdd.sortBy(lambda x: x)\n",
        "\n",
        "# Ver los resultados\n",
        "print(\"Resultado de distinct y sortBy:\")\n",
        "for value in sorted_rdd.collect():\n",
        "    print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3HYOLbgBATt",
        "outputId": "51c33a4d-80fa-4187-ad13-e40223ecfa3e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado de distinct y sortBy:\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusión\n",
        "\n",
        "Estas operaciones son solo algunos ejemplos de cómo se puede realizar un *shuffling* en Spark. Cada vez que Spark necesita redistribuir los datos entre diferentes particiones, se activa el proceso de *shuffling*, lo que puede afectar el rendimiento si no se maneja adecuadamente."
      ],
      "metadata": {
        "id": "skO_slKI_PlV"
      }
    }
  ]
}