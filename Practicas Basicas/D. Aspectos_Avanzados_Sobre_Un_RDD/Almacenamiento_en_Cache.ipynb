{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Función para instalar OpenJDK 8 si no está instalado previamente.\n",
        "def install_java():\n",
        "    # Verifica si la ruta del JDK existe.\n",
        "    if not os.path.exists('/usr/lib/jvm/java-8-openjdk-amd64'):\n",
        "        print(\"Instalando OpenJDK 8...\")\n",
        "        # Comando para instalar Java en modo silencioso.\n",
        "        !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "        print(\"OpenJDK 8 instalado correctamente.\")\n",
        "    else:\n",
        "        print(\"OpenJDK 8 ya está instalado.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para descargar Apache Spark solo si no está ya descargado.\n",
        "def download_spark():\n",
        "    # URL y nombre del archivo comprimido de Spark.\n",
        "    spark_url = \"https://archive.apache.org/dist/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\"\n",
        "    spark_tar = \"spark-3.4.3-bin-hadoop3.tgz\"\n",
        "\n",
        "    # Verifica si el archivo comprimido ya existe.\n",
        "    if not os.path.exists(spark_tar):\n",
        "        print(\"Descargando Apache Spark...\")\n",
        "        # Comando para descargar el archivo desde la URL.\n",
        "        !wget -q $spark_url\n",
        "        print(\"Descarga completa.\")\n",
        "    else:\n",
        "        print(\"El archivo de Apache Spark ya está descargado.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para descomprimir el archivo de Apache Spark solo si no está descomprimido.\n",
        "def extract_spark():\n",
        "    # Nombre de la carpeta descomprimida.\n",
        "    spark_dir = \"spark-3.4.3-bin-hadoop3\"\n",
        "    spark_tar = \"spark-3.4.3-bin-hadoop3.tgz\"\n",
        "\n",
        "    # Verifica si la carpeta descomprimida ya existe.\n",
        "    if not os.path.exists(spark_dir):\n",
        "        print(\"Descomprimiendo Apache Spark...\")\n",
        "        # Comando para descomprimir el archivo.\n",
        "        !tar xf $spark_tar\n",
        "        print(\"Apache Spark descomprimido.\")\n",
        "    else:\n",
        "        print(\"La carpeta de Apache Spark ya existe.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para configurar las variables de entorno necesarias para Spark.\n",
        "def set_environment_variables():\n",
        "    # Establece la ruta de JAVA_HOME y SPARK_HOME.\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\"\n",
        "    print(\"Variables de entorno configuradas.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para verificar si un paquete de Python ya está instalado.\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def is_package_installed(package_name):\n",
        "    try:\n",
        "        # Ejecuta el comando `pip show` para verificar si el paquete está instalado.\n",
        "        subprocess.check_call(\n",
        "            [sys.executable, \"-m\", \"pip\", \"show\", package_name],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.DEVNULL\n",
        "        )\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "# Función para instalar librerías de Python necesarias (findspark y pyspark).\n",
        "def install_python_libraries():\n",
        "    # Verifica e instala findspark.\n",
        "    if not is_package_installed(\"findspark\"):\n",
        "        print(\"Instalando findspark...\")\n",
        "        !pip install -q findspark\n",
        "    else:\n",
        "        print(\"findspark ya está instalado.\")\n",
        "\n",
        "    # Verifica e instala pyspark.\n",
        "    if not is_package_installed(\"pyspark\"):\n",
        "        print(\"Instalando pyspark...\")\n",
        "        !pip install -q pyspark\n",
        "    else:\n",
        "        print(\"pyspark ya está instalado.\")\n",
        "\n",
        "\n",
        "\n",
        "install_java()                # Instala OpenJDK 8\n",
        "download_spark()              # Descarga Apache Spark\n",
        "extract_spark()               # Descomprime Apache Spark\n",
        "set_environment_variables()   # Configura las variables de entorno\n",
        "install_python_libraries()    # Instala las librerías de Python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXwui0oU2E-x",
        "outputId": "fb2828e0-7849-4177-8e58-8a5a7cfececb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando OpenJDK 8...\n",
            "OpenJDK 8 instalado correctamente.\n",
            "Descargando Apache Spark...\n",
            "Descarga completa.\n",
            "Descomprimiendo Apache Spark...\n",
            "Apache Spark descomprimido.\n",
            "Variables de entorno configuradas.\n",
            "Instalando findspark...\n",
            "pyspark ya está instalado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **¿Qué es el almacenamiento en caché en PySpark?**\n",
        "\n",
        "En PySpark, el almacenamiento en caché (caching) permite guardar datos en la memoria para que puedan ser reutilizados en operaciones posteriores sin necesidad de recalcularlos o volver a leerlos desde el almacenamiento original. Esto mejora significativamente el rendimiento, especialmente cuando se realizan múltiples operaciones sobre los mismos datos.\n",
        "\n",
        "---\n",
        "\n",
        "### **¿Por qué usar caché?**\n",
        "1. **Evita cálculos repetidos:** Cuando trabajas con transformaciones como `filter`, `map`, o `join`, Spark recalcula todo el conjunto de datos desde el principio si no está en caché.\n",
        "2. **Acelera las operaciones:** Los datos almacenados en memoria se acceden mucho más rápido que los almacenados en disco o recomputados.\n",
        "3. **Ideal para reutilización:** Si necesitas ejecutar múltiples acciones sobre el mismo DataFrame o RDD, caché es esencial.\n",
        "\n",
        "---\n",
        "\n",
        "### **¿Cómo usar el caché en PySpark?**\n",
        "Puedes almacenar en caché un **RDD** o un **DataFrame** usando el método `cache()` o `persist()`.\n",
        "\n",
        "1. **`cache()`:** Guarda los datos en memoria (por defecto). Ejemplo:\n",
        "   ```python\n",
        "   df.cache()  # DataFrame estará en caché\n",
        "   df.show()   # Primera acción, los datos son cargados en memoria\n",
        "   ```\n",
        "\n",
        "2. **`persist(storageLevel)`:** Ofrece más control sobre cómo y dónde guardar los datos (por ejemplo, en disco o memoria). Ejemplo:\n",
        "   ```python\n",
        "   from pyspark import StorageLevel\n",
        "\n",
        "   df.persist(StorageLevel.MEMORY_AND_DISK)  # Guarda en memoria y disco\n",
        "   df.count()  # Primera acción, los datos se almacenan\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Algunos niveles de persistencia comunes:**\n",
        "- **`MEMORY_ONLY` (predeterminado):** Almacena solo en memoria. Si no cabe, los datos no se guardan.\n",
        "- **`MEMORY_AND_DISK`:** Si no hay suficiente memoria, guarda el exceso en disco.\n",
        "- **`DISK_ONLY`:** Guarda únicamente en disco (más lento).\n",
        "\n",
        "---\n",
        "\n",
        "### **Consideraciones:**\n",
        "- Usa caché solo si los datos serán reutilizados. De lo contrario, ocupa memoria innecesariamente.\n",
        "- Recuerda liberar la memoria con `unpersist()` cuando ya no necesites los datos:\n",
        "  ```python\n",
        "  df.unpersist()\n",
        "  ```"
      ],
      "metadata": {
        "id": "awRaskJhPHtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "fJED1o0aPZ3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Crear una sesion en Spark**"
      ],
      "metadata": {
        "id": "t5i-OfFAdt6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la biblioteca findspark, que ayuda a configurar PySpark correctamente en el entorno de ejecución\n",
        "import findspark\n",
        "\n",
        "# Inicializar findspark para establecer las variables necesarias de PySpark\n",
        "findspark.init()\n",
        "\n",
        "# Importar SparkSession desde la biblioteca pyspark.sql\n",
        "# SparkSession es la entrada principal para trabajar con DataFrames en PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear o obtener una SparkSession\n",
        "# Esto inicializa un entorno de Spark si no existe uno, o reutiliza uno existente\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Obtener el contexto de Spark (SparkContext) desde la SparkSession\n",
        "# SparkContext es la entrada principal para trabajar con RDDs en PySpark\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "t6iu5emrQJvB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD paralelo con los números del 0 al 9\n",
        "rdd = sc.parallelize([item for item in range(10)])\n",
        "# `sc.parallelize` distribuye la colección `[0, 1, 2, ..., 9]` en las particiones del clúster.\n",
        "# Resultado esperado: un RDD con los números del 0 al 9.\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pak3HXeCjpxg",
        "outputId": "75088a35-2cc3-42af-b2ec-277eb8075d46"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.storagelevel import StorageLevel\n",
        "# Importa el módulo `StorageLevel`, que define los niveles de persistencia para los RDD."
      ],
      "metadata": {
        "id": "x9rbOcD8j5_P"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Persistir el RDD en la memoria únicamente (MEMORY_ONLY)\n",
        "rdd.persist(StorageLevel.MEMORY_ONLY)\n",
        "# Guarda el RDD en la memoria RAM. Si la memoria es insuficiente, el resto de los datos no se guardará.\n",
        "# No se realiza ninguna acción aquí, solo se indica a Spark que lo persista cuando sea necesario.\n",
        "# Resultado esperado: Los datos estarán en memoria una vez que el RDD sea calculado."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta6A_3AKkDVP",
        "outputId": "d617cc0e-e163-4735-f708-3d895fdd74ab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:287"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar el RDD persistido de la memoria\n",
        "rdd.unpersist()\n",
        "# Libera el espacio en memoria donde estaba almacenado el RDD persistido.\n",
        "# Resultado esperado: La memoria utilizada por este RDD queda liberada."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa6Bi_uqkS6m",
        "outputId": "3b5080c8-aa43-4242-ee89-f42cf90583a9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:287"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Persistir el RDD en disco únicamente (DISK_ONLY)\n",
        "rdd.persist(StorageLevel.DISK_ONLY)\n",
        "# Almacena el RDD únicamente en el disco. Esto es más lento que la memoria, pero asegura que los datos estén disponibles incluso con poca RAM.\n",
        "# Resultado esperado: Los datos del RDD se guardarán en el disco tras la primera acción sobre este RDD."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Thim0Fg2kYXf",
        "outputId": "58aacf3c-9e5d-4841-ce92-f09c5dcce2fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:287"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar el RDD persistido del disco\n",
        "rdd.unpersist()\n",
        "# Libera el espacio en disco donde estaba almacenado el RDD persistido.\n",
        "# Resultado esperado: El almacenamiento en disco utilizado por este RDD queda liberado."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU9TrWq2klKX",
        "outputId": "5e812335-6f8d-4a7f-bd4d-c3e150db16ac"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:287"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Almacenar el RDD en memoria con el método cache()\n",
        "rdd.cache()\n",
        "# Es equivalente a `persist(StorageLevel.MEMORY_AND_DISK)`, por lo que primero intenta almacenar el RDD en memoria.\n",
        "# Si la memoria es insuficiente, guarda los datos sobrantes en el disco.\n",
        "# Resultado esperado: El RDD se almacena en memoria (y en disco si es necesario) tras la primera acción."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ayaznKxkoAO",
        "outputId": "2ac1a8ca-fc7d-4bc2-e5b1-4493849b790a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:287"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar el RDD almacenado en caché\n",
        "rdd.unpersist()\n",
        "# Libera el espacio en memoria y/o disco ocupado por el RDD cacheado.\n",
        "# Resultado esperado: Se libera cual"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6Asn9MnlfYu",
        "outputId": "a086820c-8f76-4286-926d-1d2acb47cedf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:287"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Resumen de operaciones:**\n",
        "1. Se crea un RDD distribuido con los números del 0 al 9.\n",
        "2. Se experimenta con diferentes niveles de persistencia (`MEMORY_ONLY`, `DISK_ONLY`, y `cache()`).\n",
        "3. Cada vez que se almacena en caché o se persiste un RDD, se libera después usando `unpersist()` para liberar recursos.\n",
        "\n",
        "Este código demuestra cómo manejar el almacenamiento en caché y persistencia de un RDD en Spark. Si necesitas ejemplos con acciones específicas (como `count()` o `collect()`), podemos añadirlos para observar los efectos del almacenamiento en caché. 😊"
      ],
      "metadata": {
        "id": "9qsGs2GnloWl"
      }
    }
  ]
}