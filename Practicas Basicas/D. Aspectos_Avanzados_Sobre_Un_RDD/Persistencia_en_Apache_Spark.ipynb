{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Función para instalar OpenJDK 8 si no está instalado previamente.\n",
        "def install_java():\n",
        "    # Verifica si la ruta del JDK existe.\n",
        "    if not os.path.exists('/usr/lib/jvm/java-8-openjdk-amd64'):\n",
        "        print(\"Instalando OpenJDK 8...\")\n",
        "        # Comando para instalar Java en modo silencioso.\n",
        "        !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "        print(\"OpenJDK 8 instalado correctamente.\")\n",
        "    else:\n",
        "        print(\"OpenJDK 8 ya está instalado.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para descargar Apache Spark solo si no está ya descargado.\n",
        "def download_spark():\n",
        "    # URL y nombre del archivo comprimido de Spark.\n",
        "    spark_url = \"https://archive.apache.org/dist/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\"\n",
        "    spark_tar = \"spark-3.4.3-bin-hadoop3.tgz\"\n",
        "\n",
        "    # Verifica si el archivo comprimido ya existe.\n",
        "    if not os.path.exists(spark_tar):\n",
        "        print(\"Descargando Apache Spark...\")\n",
        "        # Comando para descargar el archivo desde la URL.\n",
        "        !wget -q $spark_url\n",
        "        print(\"Descarga completa.\")\n",
        "    else:\n",
        "        print(\"El archivo de Apache Spark ya está descargado.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para descomprimir el archivo de Apache Spark solo si no está descomprimido.\n",
        "def extract_spark():\n",
        "    # Nombre de la carpeta descomprimida.\n",
        "    spark_dir = \"spark-3.4.3-bin-hadoop3\"\n",
        "    spark_tar = \"spark-3.4.3-bin-hadoop3.tgz\"\n",
        "\n",
        "    # Verifica si la carpeta descomprimida ya existe.\n",
        "    if not os.path.exists(spark_dir):\n",
        "        print(\"Descomprimiendo Apache Spark...\")\n",
        "        # Comando para descomprimir el archivo.\n",
        "        !tar xf $spark_tar\n",
        "        print(\"Apache Spark descomprimido.\")\n",
        "    else:\n",
        "        print(\"La carpeta de Apache Spark ya existe.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para configurar las variables de entorno necesarias para Spark.\n",
        "def set_environment_variables():\n",
        "    # Establece la ruta de JAVA_HOME y SPARK_HOME.\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\"\n",
        "    print(\"Variables de entorno configuradas.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para verificar si un paquete de Python ya está instalado.\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def is_package_installed(package_name):\n",
        "    try:\n",
        "        # Ejecuta el comando `pip show` para verificar si el paquete está instalado.\n",
        "        subprocess.check_call(\n",
        "            [sys.executable, \"-m\", \"pip\", \"show\", package_name],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.DEVNULL\n",
        "        )\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "# Función para instalar librerías de Python necesarias (findspark y pyspark).\n",
        "def install_python_libraries():\n",
        "    # Verifica e instala findspark.\n",
        "    if not is_package_installed(\"findspark\"):\n",
        "        print(\"Instalando findspark...\")\n",
        "        !pip install -q findspark\n",
        "    else:\n",
        "        print(\"findspark ya está instalado.\")\n",
        "\n",
        "    # Verifica e instala pyspark.\n",
        "    if not is_package_installed(\"pyspark\"):\n",
        "        print(\"Instalando pyspark...\")\n",
        "        !pip install -q pyspark\n",
        "    else:\n",
        "        print(\"pyspark ya está instalado.\")\n",
        "\n",
        "\n",
        "\n",
        "install_java()                # Instala OpenJDK 8\n",
        "download_spark()              # Descarga Apache Spark\n",
        "extract_spark()               # Descomprime Apache Spark\n",
        "set_environment_variables()   # Configura las variables de entorno\n",
        "install_python_libraries()    # Instala las librerías de Python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXwui0oU2E-x",
        "outputId": "10296ad7-3790-4804-877d-754a2ddd0b05"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenJDK 8 ya está instalado.\n",
            "El archivo de Apache Spark ya está descargado.\n",
            "La carpeta de Apache Spark ya existe.\n",
            "Variables de entorno configuradas.\n",
            "findspark ya está instalado.\n",
            "pyspark ya está instalado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persistencia en Apache Spark\n",
        "\n",
        "La **persistencia** en Apache Spark es un mecanismo que permite almacenar un **RDD** o un **DataFrame** en memoria, disco, o una combinación de ambos, para evitar recalcularlos desde cero cada vez que se necesitan. Esto es útil para mejorar el rendimiento y optimizar los recursos en aplicaciones que reutilizan datos.\n",
        "\n",
        "---\n",
        "\n",
        "## **¿Por qué usar la persistencia?**\n",
        "\n",
        "1. **Evitar cálculos innecesarios:**\n",
        "   - Spark recalcula un RDD desde su origen cada vez que es necesario.\n",
        "   - Persistir almacena el resultado intermedio, eliminando la necesidad de realizar los mismos cálculos varias veces.\n",
        "\n",
        "2. **Ahorro de recursos computacionales:**\n",
        "   - Reducir la carga en el clúster al evitar recalculaciones.\n",
        "\n",
        "3. **Mejor rendimiento:**\n",
        "   - Las operaciones sobre datos ya almacenados son más rápidas.\n",
        "\n",
        "---\n",
        "\n",
        "## **Tipos de persistencia**\n",
        "\n",
        "1. **`MEMORY_ONLY`:**\n",
        "   - Guarda los datos en la memoria RAM.\n",
        "   - Si no cabe todo en memoria, las particiones sobrantes no se almacenan.\n",
        "   - Es el nivel más rápido, pero puede ocasionar pérdida de datos si hay poca memoria.\n",
        "\n",
        "2. **`MEMORY_AND_DISK`:**\n",
        "   - Guarda los datos en memoria si es posible.\n",
        "   - Si no hay suficiente memoria, las particiones restantes se guardan en el disco.\n",
        "   - Es el nivel más común por su balance entre velocidad y estabilidad.\n",
        "\n",
        "3. **`DISK_ONLY`:**\n",
        "   - Guarda los datos exclusivamente en disco.\n",
        "   - Es más lento, pero asegura que los datos siempre estén disponibles.\n",
        "\n",
        "4. **`MEMORY_ONLY_SER`:**\n",
        "   - Similar a `MEMORY_ONLY`, pero comprime los datos en memoria para ahorrar espacio.\n",
        "   - Requiere más CPU para descomprimir.\n",
        "\n",
        "5. **`OFF_HEAP`:**\n",
        "   - Guarda los datos fuera de la pila de memoria JVM.\n",
        "   - Es útil para integraciones avanzadas con otros sistemas y evita que la JVM consuma demasiada memoria.\n",
        "\n",
        "---\n",
        "\n",
        "## **Cache vs Persistencia**\n",
        "\n",
        "- **`cache()`**:\n",
        "  - Alias de `persist(StorageLevel.MEMORY_AND_DISK)`.\n",
        "  - Práctico para la mayoría de los casos.\n",
        "\n",
        "- **`persist()`**:\n",
        "  - Permite elegir niveles de almacenamiento personalizados, como solo memoria, solo disco o combinaciones.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LOLuBc3inEi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la biblioteca findspark, que ayuda a configurar PySpark correctamente en el entorno de ejecución\n",
        "import findspark\n",
        "\n",
        "# Inicializar findspark para establecer las variables necesarias de PySpark\n",
        "findspark.init()\n",
        "\n",
        "# Importar SparkSession desde la biblioteca pyspark.sql\n",
        "# SparkSession es la entrada principal para trabajar con DataFrames en PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear o obtener una SparkSession\n",
        "# Esto inicializa un entorno de Spark si no existe uno, o reutiliza uno existente\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Obtener el contexto de Spark (SparkContext) desde la SparkSession\n",
        "# SparkContext es la entrada principal para trabajar con RDDs en PySpark\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "OttJCkjkqdST"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade pyspark\n",
        "# !pip install --upgrade cloudpickle"
      ],
      "metadata": {
        "id": "bJpTEYknpwSD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejemplos Practicos**"
      ],
      "metadata": {
        "id": "t5i-OfFAdt6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejemplo 1: Usar `MEMORY_ONLY` para datos reutilizados en memoria**"
      ],
      "metadata": {
        "id": "ViXplR4MnSWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "data = sc.parallelize(range(1, 1000000))  # Crear un RDD con un rango de números\n",
        "\n",
        "# Persistir en memoria\n",
        "filtered_data = data.filter(lambda x: x % 2 == 0).persist(StorageLevel.MEMORY_ONLY)\n",
        "\n",
        "# Primera acción (genera los datos y los guarda en memoria)\n",
        "print(filtered_data.count())\n",
        "\n",
        "# Segunda acción (reutiliza los datos desde la memoria)\n",
        "print(filtered_data.take(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wR5r39GncPs",
        "outputId": "a2e136b3-11d2-44e5-ddf2-1a3f77ab88d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "499999\n",
            "[2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Resultado esperado:** El filtrado se calcula una sola vez y las operaciones posteriores son más rápidas porque reutilizan los datos en memoria.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CWj1mwLgny4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejemplo 2: Usar `MEMORY_AND_DISK` para manejar grandes volúmenes de datos**"
      ],
      "metadata": {
        "id": "zdP3B318n3Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "# Persistir en memoria y disco\n",
        "large_data = sc.parallelize(range(1, 10000000))  # RDD con muchos datos\n",
        "\n",
        "# Almacenarlo en memoria, y si no cabe, usar el disco\n",
        "persisted_data = large_data.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "# Acción que dispara el almacenamiento\n",
        "print(persisted_data.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stf_KIwDn5Bk",
        "outputId": "04b4343f-cc3f-4bdf-a3ec-fec25980928c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49999995000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Resultado esperado:** Los datos que no caben en memoria se almacenan en disco, asegurando que todas las operaciones funcionen sin errores.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "frHHYImPoAPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejemplo 3: Cache simple con `cache()`**"
      ],
      "metadata": {
        "id": "VBH5WfYGoEfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cachear un conjunto de datos\n",
        "text_data = sc.textFile(\"/content/sample_data/california_housing_test.csv\")\n",
        "\n",
        "# Cachear para reutilización\n",
        "cached_text = text_data.cache()\n",
        "\n",
        "# Primera acción (lectura del archivo y almacenamiento en memoria y disco)\n",
        "print(cached_text.count())\n",
        "\n",
        "# Segunda acción (usa el caché y es mucho más rápida)\n",
        "print(cached_text.first())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWI9kzzwoIFc",
        "outputId": "26d365a5-2c3f-4d05-931a-707ddccae44e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3001\n",
            "\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Resultado esperado:** La primera acción guarda los datos en memoria/disco, y las acciones posteriores son más rápidas porque reutilizan el caché."
      ],
      "metadata": {
        "id": "9o-_WktooLds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Consideraciones finales**\n",
        "- **Liberar recursos:** Usa `unpersist()` para liberar memoria o disco cuando ya no necesites los datos."
      ],
      "metadata": {
        "id": "fIiIFomEoTB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cached_text.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iHxajLFoXbc",
        "outputId": "053d6e00-e38a-44c6-ef08-2a3e791a164e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/sample_data/california_housing_test.csv MapPartitionsRDD[8] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Persistir o cachear es esencial en aplicaciones donde se procesan datos reutilizables, mejorando tanto el rendimiento como la eficiencia de los recursos en Spark.\n",
        "\n",
        "Si tienes preguntas o necesitas más ejemplos, ¡estaré encantado de ayudarte! 😊\n",
        "\n"
      ],
      "metadata": {
        "id": "SCk7L_SdnP8t"
      }
    }
  ]
}