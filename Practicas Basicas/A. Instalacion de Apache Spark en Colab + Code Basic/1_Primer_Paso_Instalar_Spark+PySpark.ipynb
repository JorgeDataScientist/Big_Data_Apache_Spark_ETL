{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Paso a Paso Para Realizar Instalacion de Apache Spark en Colab**\n",
        "# **🤯**\n",
        "---"
      ],
      "metadata": {
        "id": "aLApO7WKO3kR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "z3COh-nXrene"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Función para instalar OpenJDK 8\n"
      ],
      "metadata": {
        "id": "dLWqeDnhldNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para instalar OpenJDK 8 si no está instalado previamente.\n",
        "def install_java():\n",
        "    # Verifica si la ruta del JDK existe.\n",
        "    if not os.path.exists('/usr/lib/jvm/java-8-openjdk-amd64'):\n",
        "        print(\"Instalando OpenJDK 8...\")\n",
        "        # Comando para instalar Java en modo silencioso.\n",
        "        !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "        print(\"OpenJDK 8 instalado correctamente.\")\n",
        "    else:\n",
        "        print(\"OpenJDK 8 ya está instalado.\")"
      ],
      "metadata": {
        "id": "ocwoPb91loee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Función para descargar Apache Spark"
      ],
      "metadata": {
        "id": "avmuQpDIlrk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para descargar Apache Spark solo si no está ya descargado.\n",
        "def download_spark():\n",
        "    # URL y nombre del archivo comprimido de Spark.\n",
        "    spark_url = \"https://archive.apache.org/dist/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\"\n",
        "    spark_tar = \"spark-3.4.3-bin-hadoop3.tgz\"\n",
        "\n",
        "    # Verifica si el archivo comprimido ya existe.\n",
        "    if not os.path.exists(spark_tar):\n",
        "        print(\"Descargando Apache Spark...\")\n",
        "        # Comando para descargar el archivo desde la URL.\n",
        "        !wget -q $spark_url\n",
        "        print(\"Descarga completa.\")\n",
        "    else:\n",
        "        print(\"El archivo de Apache Spark ya está descargado.\")"
      ],
      "metadata": {
        "id": "Wt6hZEyulx52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Función para descomprimir Apache Spark"
      ],
      "metadata": {
        "id": "8Jgxtdhal1MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para descomprimir el archivo de Apache Spark solo si no está descomprimido.\n",
        "def extract_spark():\n",
        "    # Nombre de la carpeta descomprimida.\n",
        "    spark_dir = \"spark-3.4.3-bin-hadoop3\"\n",
        "    spark_tar = \"spark-3.4.3-bin-hadoop3.tgz\"\n",
        "\n",
        "    # Verifica si la carpeta descomprimida ya existe.\n",
        "    if not os.path.exists(spark_dir):\n",
        "        print(\"Descomprimiendo Apache Spark...\")\n",
        "        # Comando para descomprimir el archivo.\n",
        "        !tar xf $spark_tar\n",
        "        print(\"Apache Spark descomprimido.\")\n",
        "    else:\n",
        "        print(\"La carpeta de Apache Spark ya existe.\")"
      ],
      "metadata": {
        "id": "ixXnx5VBl5mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Función para configurar variables de entorno"
      ],
      "metadata": {
        "id": "CUL79waXl8G2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para configurar las variables de entorno necesarias para Spark.\n",
        "def set_environment_variables():\n",
        "    # Establece la ruta de JAVA_HOME y SPARK_HOME.\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\"\n",
        "    print(\"Variables de entorno configuradas.\")"
      ],
      "metadata": {
        "id": "9Z1obZ1al_Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Función para verificar si un paquete está instalado"
      ],
      "metadata": {
        "id": "5lZztVgNmErm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para verificar si un paquete de Python ya está instalado.\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def is_package_installed(package_name):\n",
        "    try:\n",
        "        # Ejecuta el comando `pip show` para verificar si el paquete está instalado.\n",
        "        subprocess.check_call(\n",
        "            [sys.executable, \"-m\", \"pip\", \"show\", package_name],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.DEVNULL\n",
        "        )\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        return False"
      ],
      "metadata": {
        "id": "l7iVbLSFmL_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Función para instalar findspark y pyspark"
      ],
      "metadata": {
        "id": "pGJ6jvDVmSBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para instalar librerías de Python necesarias (findspark y pyspark).\n",
        "def install_python_libraries():\n",
        "    # Verifica e instala findspark.\n",
        "    if not is_package_installed(\"findspark\"):\n",
        "        print(\"Instalando findspark...\")\n",
        "        !pip install -q findspark\n",
        "    else:\n",
        "        print(\"findspark ya está instalado.\")\n",
        "\n",
        "    # Verifica e instala pyspark.\n",
        "    if not is_package_installed(\"pyspark\"):\n",
        "        print(\"Instalando pyspark...\")\n",
        "        !pip install -q pyspark\n",
        "    else:\n",
        "        print(\"pyspark ya está instalado.\")"
      ],
      "metadata": {
        "id": "iFauCFnDmT7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Llamar a las funciones**"
      ],
      "metadata": {
        "id": "5aOgcvM-mZam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install_java()                # Instala OpenJDK 8\n",
        "download_spark()              # Descarga Apache Spark\n",
        "extract_spark()               # Descomprime Apache Spark\n",
        "set_environment_variables()   # Configura las variables de entorno\n",
        "install_python_libraries()    # Instala las librerías de Python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9NdiQLAmfPW",
        "outputId": "44f627c3-4ad6-4662-8e10-d2ac363ae8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenJDK 8 ya está instalado.\n",
            "El archivo de Apache Spark ya está descargado.\n",
            "La carpeta de Apache Spark ya existe.\n",
            "Variables de entorno configuradas.\n",
            "findspark ya está instalado.\n",
            "pyspark ya está instalado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "O24lzVStmn72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Verificar la Instalacion"
      ],
      "metadata": {
        "id": "qZTO7-eNaT0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos la biblioteca findspark\n",
        "import findspark\n",
        "\n",
        "# Inicializamos findspark para configurar las rutas necesarias para Spark\n",
        "findspark.init()\n",
        "\n",
        "# Importamos SparkSession desde pyspark.sql\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Creamos una instancia de SparkSession\n",
        "# - `master(\"local[*]\")`: Ejecuta Spark en modo local usando todos los núcleos disponibles.\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "LFws1CIIVcVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un DataFrame de ejemplo con varias columnas\n",
        "# - `datos`: Es una lista de tuplas, donde cada tupla representa una fila del DataFrame.\n",
        "# - `columnas`: Especifica los nombres de las columnas.\n",
        "datos = [\n",
        "    (\"Jorge Luis\", 46, \"Científico de Datos\"),\n",
        "    (\"Maria Gabriela\", 41, \"Administrador\"),\n",
        "    (\"Barbara\", 10, \"Pintor\"),\n",
        "    (\"Marco Antonio\", 5, \"Pre-Esccolar\"),\n",
        "    (\"Luis\", 74, \"Escritor\")\n",
        "]\n",
        "columnas = [\"Nombre\", \"Edad\", \"Profesión\"]\n",
        "\n",
        "# Creamos el DataFrame utilizando los datos y las columnas especificadas\n",
        "df = spark.createDataFrame(datos, columnas)\n",
        "\n",
        "# Mostramos el contenido del DataFrame\n",
        "# - `show()`: Muestra las primeras filas del DataFrame.\n",
        "# - `truncate=False`: Evita truncar los valores largos en las columnas.\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90Ru8UsGXFZA",
        "outputId": "b1fa3204-165f-42a5-f1d7-95d543d14797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----+-------------------+\n",
            "|Nombre        |Edad|Profesión          |\n",
            "+--------------+----+-------------------+\n",
            "|Jorge Luis    |46  |Científico de Datos|\n",
            "|Maria Gabriela|41  |Administrador      |\n",
            "|Barbara       |10  |Pintor             |\n",
            "|Marco Antonio |5   |Pre-Esccolar       |\n",
            "|Luis          |74  |Escritor           |\n",
            "+--------------+----+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 👌 🆗 **Culminado**\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "3GIDgjm2o_CW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Diferentes Formas de Crear un RDD**\n",
        "\n",
        "# 🔖"
      ],
      "metadata": {
        "id": "kP5vxailckCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅"
      ],
      "metadata": {
        "id": "t6TFiq6gdGku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos la biblioteca findspark\n",
        "import findspark\n",
        "\n",
        "# Inicializamos findspark para configurar las rutas necesarias para Spark\n",
        "findspark.init()\n",
        "\n",
        "# Importamos SparkSession desde pyspark.sql\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Creamos una instancia de SparkSession\n",
        "# - `master(\"local[*]\")`: Ejecuta Spark en modo local usando todos los núcleos disponibles.\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName('Curso Pyspark').getOrCreate()"
      ],
      "metadata": {
        "id": "QQOGtTmocwW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD a partir de un DF\n",
        "\n",
        "# Crear un DataFrame de ejemplo con varias columnas\n",
        "# - `datos`: Es una lista de tuplas, donde cada tupla representa una fila del DataFrame.\n",
        "# - `columnas`: Especifica los nombres de las columnas.\n",
        "datos = [\n",
        "    (\"Jorge Luis\", 46, \"Científico de Datos\"),\n",
        "    (\"Maria Gabriela\", 41, \"Administrador\"),\n",
        "    (\"Barbara\", 10, \"Pintor\"),\n",
        "    (\"Marco Antonio\", 5, \"Pre-Esccolar\"),\n",
        "    (\"Luis\", 74, \"Escritor\")\n",
        "]\n",
        "columnas = [\"Nombre\", \"Edad\", \"Profesión\"]\n",
        "\n",
        "# Creamos el DataFrame utilizando los datos y las columnas especificadas\n",
        "df = spark.createDataFrame(datos, columnas)\n",
        "\n",
        "# Mostramos el contenido del DataFrame\n",
        "# - `show()`: Muestra las primeras filas del DataFrame.\n",
        "# - `truncate=False`: Evita truncar los valores largos en las columnas.\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQgysix3dE9W",
        "outputId": "eefaa086-d1d4-4c69-f3dc-d3069c538d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----+-------------------+\n",
            "|Nombre        |Edad|Profesión          |\n",
            "+--------------+----+-------------------+\n",
            "|Jorge Luis    |46  |Científico de Datos|\n",
            "|Maria Gabriela|41  |Administrador      |\n",
            "|Barbara       |10  |Pintor             |\n",
            "|Marco Antonio |5   |Pre-Esccolar       |\n",
            "|Luis          |74  |Escritor           |\n",
            "+--------------+----+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅"
      ],
      "metadata": {
        "id": "feP6-fc7dk7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el contexto de Spark (SparkContext) desde la sesión de Spark\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "iqOcDGo4dwhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "5AIb8mp2eL8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD vacio\n",
        "rdd_vacio_1 = sc.emptyRDD"
      ],
      "metadata": {
        "id": "rva6BFjqdyW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "_9Iujw33eNpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD vacio con parallelize y un numero de particiones de 3\n",
        "rdd_vacio_2 = sc.parallelize([], 3)\n",
        "rdd_vacio_2.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deYEdjFad1Fm",
        "outputId": "1d64bf86-8d98-4efc-c5ba-49dfec877fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "fRFAsMq1eBMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con datos, con parallelize y un numero de particiones de 3\n",
        "rdd_con_datos = sc.parallelize([1, 2, 3, 4, 5, 6], 3)\n",
        "\n",
        "rdd_con_datos.getNumPartitions() # numero de particiones"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukpA2Vivd38f",
        "outputId": "9fea8d23-3d11-4093-8c5c-610242dd6e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_con_datos # mostrar el numero de objetos construidos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXowX7ggd9pu",
        "outputId": "3f8c88e4-5786-461d-bdc2-18cfdc01956f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[49] at readRDDFromFile at PythonRDD.scala:287"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_con_datos.collect() # Verificar el contenido del rdd_vacio_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxdhLlkEeRam",
        "outputId": "f451f0ac-d3f2-4956-8292-4a1f74361206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "A3tceFmPeH8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD desde un archivo de texto\n",
        "rdd_texto = sc.textFile('/content/rdd_source.txt')\n",
        "\n",
        "# ver el contenido\n",
        "rdd_texto.collect()\n",
        "\n",
        "# En este caso, cada linea de texto (revisa el archivo de texto) representa un registo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDhQj5ImefnH",
        "outputId": "14d7e5e7-3227-40f0-cdae-a628ac8f1592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Así podemos crear', 'un RDD desde un', 'archivo de texto!!!']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "83xNJjHbeUi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD desde un archivo de texto en donde todo el archivo de texto este en un solo registro\n",
        "rdd_texto_2 = sc.wholeTextFiles('/content/sample_data/rdd_source.txt')\n",
        "rdd_texto_2.collect()\n",
        "\n",
        "# La diferencia es que en un solo registro se encuentra todo el texto del archivo de texto.\n",
        "# En la primera linea de la respuesta se ve el nombre del archivo donde esta el registro y luego el contenido del archivo de texto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHsisDC5fCD-",
        "outputId": "7abd676e-13dd-4348-9b62-51e2bdfe918a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('file:/content/sample_data/rdd_source.txt',\n",
              "  'Así podemos crear\\nun RDD desde un\\narchivo de texto!!!')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Crear un RDD a partir de otro existente"
      ],
      "metadata": {
        "id": "d6jfWOU6fHuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD (Resilient Distributed Dataset) a partir de una lista de Python\n",
        "resultado_rdd = sc.parallelize([item for item in range(10)])  # Crear un RDD\n",
        "\n",
        "# Mostrar el resultado usando collect\n",
        "resultado = resultado_rdd.collect()\n",
        "print(\"Resultado original:\", resultado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhoYNuFLfQ8N",
        "outputId": "1b1a7edf-6cdf-47aa-b507-bff21fb22550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado original: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un nuevo RDD a partir del existente, sumando 1 a cada elemento\n",
        "rdd_suma = resultado_rdd.map(lambda x: x + 1)\n",
        "\n",
        "# Recoger los resultados del RDD modificado y mostrarlos\n",
        "resultado_suma = rdd_suma.collect()\n",
        "print(\"Resultado tras sumar 1:\", resultado_suma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UeDupwKfVEn",
        "outputId": "1e2f2415-0523-4e31-a4b2-45cf4d65a188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado tras sumar 1: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 👌 🆗 **Culminado**\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "a1T5iHvivkHT"
      }
    }
  ]
}