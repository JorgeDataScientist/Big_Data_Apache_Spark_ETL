{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Función para instalar OpenJDK 8 si no está instalado previamente.\n",
        "def install_java():\n",
        "    # Verifica si la ruta del JDK existe.\n",
        "    if not os.path.exists('/usr/lib/jvm/java-8-openjdk-amd64'):\n",
        "        print(\"Instalando OpenJDK 8...\")\n",
        "        # Comando para instalar Java en modo silencioso.\n",
        "        !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "        print(\"OpenJDK 8 instalado correctamente.\")\n",
        "    else:\n",
        "        print(\"OpenJDK 8 ya está instalado.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para descargar Apache Spark solo si no está ya descargado.\n",
        "def download_spark():\n",
        "    # URL y nombre del archivo comprimido de Spark.\n",
        "    spark_url = \"https://archive.apache.org/dist/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\"\n",
        "    spark_tar = \"spark-3.4.3-bin-hadoop3.tgz\"\n",
        "\n",
        "    # Verifica si el archivo comprimido ya existe.\n",
        "    if not os.path.exists(spark_tar):\n",
        "        print(\"Descargando Apache Spark...\")\n",
        "        # Comando para descargar el archivo desde la URL.\n",
        "        !wget -q $spark_url\n",
        "        print(\"Descarga completa.\")\n",
        "    else:\n",
        "        print(\"El archivo de Apache Spark ya está descargado.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para descomprimir el archivo de Apache Spark solo si no está descomprimido.\n",
        "def extract_spark():\n",
        "    # Nombre de la carpeta descomprimida.\n",
        "    spark_dir = \"spark-3.4.3-bin-hadoop3\"\n",
        "    spark_tar = \"spark-3.4.3-bin-hadoop3.tgz\"\n",
        "\n",
        "    # Verifica si la carpeta descomprimida ya existe.\n",
        "    if not os.path.exists(spark_dir):\n",
        "        print(\"Descomprimiendo Apache Spark...\")\n",
        "        # Comando para descomprimir el archivo.\n",
        "        !tar xf $spark_tar\n",
        "        print(\"Apache Spark descomprimido.\")\n",
        "    else:\n",
        "        print(\"La carpeta de Apache Spark ya existe.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para configurar las variables de entorno necesarias para Spark.\n",
        "def set_environment_variables():\n",
        "    # Establece la ruta de JAVA_HOME y SPARK_HOME.\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\"\n",
        "    print(\"Variables de entorno configuradas.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para verificar si un paquete de Python ya está instalado.\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def is_package_installed(package_name):\n",
        "    try:\n",
        "        # Ejecuta el comando `pip show` para verificar si el paquete está instalado.\n",
        "        subprocess.check_call(\n",
        "            [sys.executable, \"-m\", \"pip\", \"show\", package_name],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.DEVNULL\n",
        "        )\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "# Función para instalar librerías de Python necesarias (findspark y pyspark).\n",
        "def install_python_libraries():\n",
        "    # Verifica e instala findspark.\n",
        "    if not is_package_installed(\"findspark\"):\n",
        "        print(\"Instalando findspark...\")\n",
        "        !pip install -q findspark\n",
        "    else:\n",
        "        print(\"findspark ya está instalado.\")\n",
        "\n",
        "    # Verifica e instala pyspark.\n",
        "    if not is_package_installed(\"pyspark\"):\n",
        "        print(\"Instalando pyspark...\")\n",
        "        !pip install -q pyspark\n",
        "    else:\n",
        "        print(\"pyspark ya está instalado.\")\n",
        "\n",
        "\n",
        "\n",
        "install_java()                # Instala OpenJDK 8\n",
        "download_spark()              # Descarga Apache Spark\n",
        "extract_spark()               # Descomprime Apache Spark\n",
        "set_environment_variables()   # Configura las variables de entorno\n",
        "install_python_libraries()    # Instala las librerías de Python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXwui0oU2E-x",
        "outputId": "1eb69749-4f76-443b-e2b2-51a91179cb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenJDK 8 ya está instalado.\n",
            "El archivo de Apache Spark ya está descargado.\n",
            "La carpeta de Apache Spark ya existe.\n",
            "Variables de entorno configuradas.\n",
            "findspark ya está instalado.\n",
            "pyspark ya está instalado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xfnNZQ_UxxNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicios de Introduccion a los RDD:**\n",
        "\n",
        "# ⚡"
      ],
      "metadata": {
        "id": "1pxLXmafwydL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Cree una sesión de Spark con nombre Cap2 y asegúrese de que emplea todos los cores disponibles para ejecutar en su ambiente de trabajo"
      ],
      "metadata": {
        "id": "w-x9sSXOzn5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos la biblioteca findspark, que ayuda a configurar PySpark en el entorno de Python.\n",
        "# Findspark asegura que el programa pueda localizar la instalación de Apache Spark en el sistema.\n",
        "import findspark\n",
        "\n",
        "# Inicializamos findspark para que configure automáticamente las variables necesarias\n",
        "# y encuentre la instalación de Spark en el sistema. Este paso es útil especialmente\n",
        "# si Spark no está configurado globalmente en las variables de entorno del sistema.\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "ColcyO0y2yhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos SparkSession, que es la entrada principal para usar Spark SQL y trabajar con datos estructurados.\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Creamos una sesión de Spark. Este objeto nos permite interactuar con el framework Apache Spark.\n",
        "# builder: Es un método para configurar la sesión.\n",
        "# master(\"local[*]\"): Define dónde se ejecutará el procesamiento.\n",
        "#    - \"local[*]\" significa que se usará el modo local y se aprovecharán todos los núcleos del procesador disponibles.\n",
        "# appName('Curso Pyspark'): Especifica un nombre para la aplicación Spark. Este nombre se verá en la interfaz web de Spark.\n",
        "# getOrCreate(): Si ya existe una sesión con la misma configuración, la reutiliza. Si no, crea una nueva.\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName('Cap2').getOrCreate()\n",
        "\n",
        "# Mostramos la sesión creada. Esto imprime información básica sobre la configuración y el entorno de Spark.\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "zDBfKnNL3WQQ",
        "outputId": "d5114a3d-ba7f-4108-80e3-2ffc161cea93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7d8a86348390>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7753475e8546:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Cap2</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cree dos RDD vacíos, uno de ellos no debe contener particiones y el otro debe tener 5 particiones. Utilice vías diferentes para crear cada RDD."
      ],
      "metadata": {
        "id": "E1q4WNAMz66p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD vacío sin particiones\n",
        "# `emptyRDD()` crea un RDD completamente vacío, sin ningún dato.\n",
        "# Este RDD no tendrá particiones explícitas, ya que no se especifica el número.\n",
        "rdd_sin_particiones = spark.sparkContext.emptyRDD()\n",
        "\n",
        "# Mostrar las particiones del RDD\n",
        "# `getNumPartitions()` devuelve el número de particiones del RDD.\n",
        "# Como no se especificaron particiones al crear este RDD, por defecto será una sola partición.\n",
        "print(\"Número de particiones en rdd_sin_particiones:\", rdd_sin_particiones.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45kUS9Jr55FA",
        "outputId": "7b4ddbd6-81cf-4aae-be4a-0c0de61a51b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de particiones en rdd_sin_particiones: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD vacío con 5 particiones\n",
        "# `parallelize([], 5)` crea un RDD vacío (sin datos) especificando que debe tener 5 particiones.\n",
        "# Esto asegura que, aunque el RDD no contenga datos, estará dividido en 5 particiones,\n",
        "# lo cual puede ser útil para mantener una estructura consistente para operaciones futuras.\n",
        "rdd_con_particiones = spark.sparkContext.parallelize([], 5)\n",
        "\n",
        "# Mostrar las particiones del RDD\n",
        "# `getNumPartitions()` se utiliza para consultar cuántas particiones tiene el RDD.\n",
        "# Este paso confirma que el RDD fue creado correctamente con el número de particiones especificado.\n",
        "print(\"Número de particiones en rdd_con_particiones:\", rdd_con_particiones.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puEhK7FF7VIH",
        "outputId": "a8223c19-4f3e-4253-c7b0-65e362eef3d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de particiones en rdd_con_particiones: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Cree un RDD que contenga los números primos que hay entre 1 y 20."
      ],
      "metadata": {
        "id": "_fbC92_A0AzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "# Aquí se inicia una sesión Spark para trabajar con RDDs.\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Números Primos\").getOrCreate()\n",
        "\n",
        "# Crear un SparkContext\n",
        "# SparkContext es necesario para trabajar directamente con RDDs.\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Función auxiliar para verificar si un número es primo\n",
        "# Esta función toma un número `n` y devuelve True si es primo, de lo contrario, False.\n",
        "def es_primo(n):\n",
        "    # Los números menores que 2 no son primos\n",
        "    if n < 2:\n",
        "        return False\n",
        "    # Verificamos divisores desde 2 hasta la raíz cuadrada del número\n",
        "    for i in range(2, int(n**0.5) + 1):\n",
        "        if n % i == 0:  # Si `n` es divisible por `i`, no es primo\n",
        "            return False\n",
        "    return True  # Si no se encontraron divisores, el número es primo\n",
        "\n",
        "# Crear un RDD con los números del 1 al 20\n",
        "# `parallelize` toma una lista y la convierte en un RDD que Spark puede procesar.\n",
        "rdd_numeros = sc.parallelize(range(1, 21))\n",
        "\n",
        "# Filtrar los números primos\n",
        "# Usamos `filter` para aplicar la función `es_primo` a cada elemento del RDD.\n",
        "# Solo los elementos para los cuales `es_primo` devuelve True permanecerán en el RDD resultante.\n",
        "rdd_primos = rdd_numeros.filter(es_primo)\n",
        "\n",
        "# Recoger los resultados del RDD y mostrarlos\n",
        "# `collect` devuelve todos los elementos del RDD como una lista en el controlador (entorno local).\n",
        "resultado_primos = rdd_primos.collect()\n",
        "print(\"Números primos entre 1 y 20:\", resultado_primos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_trA2nO-tPv",
        "outputId": "1e725cd0-7693-4b8b-e19d-bc44d680497b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Números primos entre 1 y 20: [2, 3, 5, 7, 11, 13, 17, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Cree un nuevo RDD a partir del RDD creado en el ejercicio anterior el cuál solo contenga los números primos mayores a 10."
      ],
      "metadata": {
        "id": "jyaAlwaY0H2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar los números primos mayores a 10\n",
        "# Utilizamos `filter` nuevamente, pero esta vez solo dejamos los números mayores a 10.\n",
        "rdd_primos_mayores_a_10 = rdd_primos.filter(lambda x: x > 10)\n",
        "\n",
        "# Recoger los resultados del nuevo RDD y mostrarlos\n",
        "# `collect` devuelve todos los elementos del RDD como una lista en el controlador (entorno local).\n",
        "resultado_primos_mayores_a_10 = rdd_primos_mayores_a_10.collect()\n",
        "\n",
        "# Mostrar los números primos mayores a 10\n",
        "print(\"Números primos mayores a 10:\", resultado_primos_mayores_a_10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqKPCLt1HwUd",
        "outputId": "b66e6359-6343-4e77-8ea9-2007fefe44b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Números primos mayores a 10: [11, 13, 17, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Descargue el archivo de texto adjunto a esta lección como recurso y guárdelo en una carpeta llamada data en el ambiente de trabajo de Colab.\n",
        "\n",
        "\n",
        "*   Cree un RDD a partir de este archivo de texto en donde todo el documento esté contenido en ***`un solo`*** registro. ¿Cómo podría saber la dirección donde está guardado el archivo de texto a partir del RDD creado?"
      ],
      "metadata": {
        "id": "6LUbs_Mo0JwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD desde un archivo de texto en donde todo el archivo de texto este en un solo registro\n",
        "rdd_texto_2 = sc.wholeTextFiles('/content/sample_data/el_valor_del_big_data.txt')\n",
        "rdd_texto_2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4O9c67eAnxR",
        "outputId": "dfe1698b-5a67-49b3-dc82-962c03db4627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('file:/content/sample_data/el_valor_del_big_data.txt',\n",
              "  'El valor y la realidad de big data\\r\\nEn los últimos años, han surgido otras \"dos V\": valor y veracidad. Los datos poseen un valor intrínseco. Sin embargo, no tienen ninguna utilidad hasta que dicho valor se descubre. Resulta igualmente importante: ¿cuál es la veracidad de sus datos y cuánto puede confiar en ellos?\\r\\n\\r\\n--\\r\\n\\r\\nHoy en día, el big data se ha convertido en un activo crucial. Piense en algunas de las mayores empresas tecnológicas del mundo. Gran parte del valor que ofrecen procede de sus datos, que analizan constantemente para generar una mayor eficiencia y desarrollar nuevos productos.\\r\\n\\r\\nAvances tecnológicos recientes han reducido exponencialmente el coste del almacenamiento y la computación de datos, haciendo que almacenar datos resulte más fácil y barato que nunca. Actualmente, con un mayor volumen de big data más barato y accesible, puede tomar decisiones empresariales más acertadas y precisas.\\r\\n\\r\\n--\\r\\n\\r\\nIdentificar el valor del big data no pasa solo por analizarlo (que es ya una ventaja en sí misma). Se trata de todo un proceso de descubrimiento que requiere que los analistas, usuarios empresariales y ejecutivos se planteen las preguntas correctas, identifiquen patrones, formulen hipótesis informadas y predigan comportamientos.')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "I3TqmpSRAkzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cree un RDD a partir de este archivo de texto en donde todo el documento esté contenido en **`varios`** registro. ¿Cómo podría saber la dirección donde está guardado el archivo de texto a partir del RDD creado?"
      ],
      "metadata": {
        "id": "Bg0VAUOKAQFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD desde un archivo de texto\n",
        "rdd_texto = sc.textFile('/content/sample_data/el_valor_del_big_data.txt')\n",
        "\n",
        "# ver el contenido\n",
        "rdd_texto.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzBgToNF5eyt",
        "outputId": "95c25ea0-da94-4e86-87d0-76f3be42958b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['El valor y la realidad de big data',\n",
              " 'En los últimos años, han surgido otras \"dos V\": valor y veracidad. Los datos poseen un valor intrínseco. Sin embargo, no tienen ninguna utilidad hasta que dicho valor se descubre. Resulta igualmente importante: ¿cuál es la veracidad de sus datos y cuánto puede confiar en ellos?',\n",
              " '',\n",
              " '--',\n",
              " '',\n",
              " 'Hoy en día, el big data se ha convertido en un activo crucial. Piense en algunas de las mayores empresas tecnológicas del mundo. Gran parte del valor que ofrecen procede de sus datos, que analizan constantemente para generar una mayor eficiencia y desarrollar nuevos productos.',\n",
              " '',\n",
              " 'Avances tecnológicos recientes han reducido exponencialmente el coste del almacenamiento y la computación de datos, haciendo que almacenar datos resulte más fácil y barato que nunca. Actualmente, con un mayor volumen de big data más barato y accesible, puede tomar decisiones empresariales más acertadas y precisas.',\n",
              " '',\n",
              " '--',\n",
              " '',\n",
              " 'Identificar el valor del big data no pasa solo por analizarlo (que es ya una ventaja en sí misma). Se trata de todo un proceso de descubrimiento que requiere que los analistas, usuarios empresariales y ejecutivos se planteen las preguntas correctas, identifiquen patrones, formulen hipótesis informadas y predigan comportamientos.']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Si necesitara crear un RDD a partir del archivo de texto cargado previamente en donde cada línea del archivo fuera un registro del RDD, ¿cómo lo haría?"
      ],
      "metadata": {
        "id": "xuavnRk7_yi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraer el contenido del archivo del RDD original y dividirlo en líneas\n",
        "rdd_lineas = rdd_texto_2.flatMap(lambda x: x[1].splitlines())\n",
        "\n",
        "# Mostrar las líneas del nuevo RDD\n",
        "resultado_lineas = rdd_lineas.collect()\n",
        "print(\"Contenido del RDD con cada línea como un registro:\")\n",
        "for linea in resultado_lineas:\n",
        "    print(linea)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhDpUWVZBstp",
        "outputId": "c3711596-5024-4798-ac86-01c304af9465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contenido del RDD con cada línea como un registro:\n",
            "El valor y la realidad de big data\n",
            "En los últimos años, han surgido otras \"dos V\": valor y veracidad. Los datos poseen un valor intrínseco. Sin embargo, no tienen ninguna utilidad hasta que dicho valor se descubre. Resulta igualmente importante: ¿cuál es la veracidad de sus datos y cuánto puede confiar en ellos?\n",
            "\n",
            "--\n",
            "\n",
            "Hoy en día, el big data se ha convertido en un activo crucial. Piense en algunas de las mayores empresas tecnológicas del mundo. Gran parte del valor que ofrecen procede de sus datos, que analizan constantemente para generar una mayor eficiencia y desarrollar nuevos productos.\n",
            "\n",
            "Avances tecnológicos recientes han reducido exponencialmente el coste del almacenamiento y la computación de datos, haciendo que almacenar datos resulte más fácil y barato que nunca. Actualmente, con un mayor volumen de big data más barato y accesible, puede tomar decisiones empresariales más acertadas y precisas.\n",
            "\n",
            "--\n",
            "\n",
            "Identificar el valor del big data no pasa solo por analizarlo (que es ya una ventaja en sí misma). Se trata de todo un proceso de descubrimiento que requiere que los analistas, usuarios empresariales y ejecutivos se planteen las preguntas correctas, identifiquen patrones, formulen hipótesis informadas y predigan comportamientos.\n"
          ]
        }
      ]
    }
  ]
}