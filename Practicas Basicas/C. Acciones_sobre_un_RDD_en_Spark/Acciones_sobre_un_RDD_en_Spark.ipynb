{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Función para instalar OpenJDK 8 si no está instalado previamente.\n",
        "def install_java():\n",
        "    # Verifica si la ruta del JDK existe.\n",
        "    if not os.path.exists('/usr/lib/jvm/java-8-openjdk-amd64'):\n",
        "        print(\"Instalando OpenJDK 8...\")\n",
        "        # Comando para instalar Java en modo silencioso.\n",
        "        !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "        print(\"OpenJDK 8 instalado correctamente.\")\n",
        "    else:\n",
        "        print(\"OpenJDK 8 ya está instalado.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para descargar Apache Spark solo si no está ya descargado.\n",
        "def download_spark():\n",
        "    # URL y nombre del archivo comprimido de Spark.\n",
        "    spark_url = \"https://archive.apache.org/dist/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\"\n",
        "    spark_tar = \"spark-3.4.3-bin-hadoop3.tgz\"\n",
        "\n",
        "    # Verifica si el archivo comprimido ya existe.\n",
        "    if not os.path.exists(spark_tar):\n",
        "        print(\"Descargando Apache Spark...\")\n",
        "        # Comando para descargar el archivo desde la URL.\n",
        "        !wget -q $spark_url\n",
        "        print(\"Descarga completa.\")\n",
        "    else:\n",
        "        print(\"El archivo de Apache Spark ya está descargado.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para descomprimir el archivo de Apache Spark solo si no está descomprimido.\n",
        "def extract_spark():\n",
        "    # Nombre de la carpeta descomprimida.\n",
        "    spark_dir = \"spark-3.4.3-bin-hadoop3\"\n",
        "    spark_tar = \"spark-3.4.3-bin-hadoop3.tgz\"\n",
        "\n",
        "    # Verifica si la carpeta descomprimida ya existe.\n",
        "    if not os.path.exists(spark_dir):\n",
        "        print(\"Descomprimiendo Apache Spark...\")\n",
        "        # Comando para descomprimir el archivo.\n",
        "        !tar xf $spark_tar\n",
        "        print(\"Apache Spark descomprimido.\")\n",
        "    else:\n",
        "        print(\"La carpeta de Apache Spark ya existe.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para configurar las variables de entorno necesarias para Spark.\n",
        "def set_environment_variables():\n",
        "    # Establece la ruta de JAVA_HOME y SPARK_HOME.\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\"\n",
        "    print(\"Variables de entorno configuradas.\")\n",
        "\n",
        "\n",
        "\n",
        "# Función para verificar si un paquete de Python ya está instalado.\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def is_package_installed(package_name):\n",
        "    try:\n",
        "        # Ejecuta el comando `pip show` para verificar si el paquete está instalado.\n",
        "        subprocess.check_call(\n",
        "            [sys.executable, \"-m\", \"pip\", \"show\", package_name],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.DEVNULL\n",
        "        )\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "# Función para instalar librerías de Python necesarias (findspark y pyspark).\n",
        "def install_python_libraries():\n",
        "    # Verifica e instala findspark.\n",
        "    if not is_package_installed(\"findspark\"):\n",
        "        print(\"Instalando findspark...\")\n",
        "        !pip install -q findspark\n",
        "    else:\n",
        "        print(\"findspark ya está instalado.\")\n",
        "\n",
        "    # Verifica e instala pyspark.\n",
        "    if not is_package_installed(\"pyspark\"):\n",
        "        print(\"Instalando pyspark...\")\n",
        "        !pip install -q pyspark\n",
        "    else:\n",
        "        print(\"pyspark ya está instalado.\")\n",
        "\n",
        "\n",
        "\n",
        "install_java()                # Instala OpenJDK 8\n",
        "download_spark()              # Descarga Apache Spark\n",
        "extract_spark()               # Descomprime Apache Spark\n",
        "set_environment_variables()   # Configura las variables de entorno\n",
        "install_python_libraries()    # Instala las librerías de Python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXwui0oU2E-x",
        "outputId": "8abd555c-f97f-4d12-8a7e-ce47346ea056"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenJDK 8 ya está instalado.\n",
            "El archivo de Apache Spark ya está descargado.\n",
            "La carpeta de Apache Spark ya existe.\n",
            "Variables de entorno configuradas.\n",
            "findspark ya está instalado.\n",
            "pyspark ya está instalado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **Crear una sesion en Spark**"
      ],
      "metadata": {
        "id": "t5i-OfFAdt6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la biblioteca findspark, que ayuda a configurar PySpark correctamente en el entorno de ejecución\n",
        "import findspark\n",
        "\n",
        "# Inicializar findspark para establecer las variables necesarias de PySpark\n",
        "findspark.init()\n",
        "\n",
        "# Importar SparkSession desde la biblioteca pyspark.sql\n",
        "# SparkSession es la entrada principal para trabajar con DataFrames en PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear o obtener una SparkSession\n",
        "# Esto inicializa un entorno de Spark si no existe uno, o reutiliza uno existente\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Obtener el contexto de Spark (SparkContext) desde la SparkSession\n",
        "# SparkContext es la entrada principal para trabajar con RDDs en PySpark\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "eztf5DxddjDm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "plqpk_hBd2J7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funcion `reduce`"
      ],
      "metadata": {
        "id": "laaq27Y-dK3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD (Resilient Distributed Dataset) con los números 1, 2, 3, 4, 5\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# Usar la función reduce para sumar todos los elementos del RDD\n",
        "# La función lambda toma dos argumentos (x, y) y devuelve su suma\n",
        "resultado = rdd.reduce(lambda x, y: x + y)\n",
        "\n",
        "# Imprimir el resultado de la suma\n",
        "print(resultado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8GdPB84eaKl",
        "outputId": "0324eb44-1a63-47f6-8fdf-353fc6d90627"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funcion `map`"
      ],
      "metadata": {
        "id": "e71SS2G8dYuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con los números 1, 2, 3, 4, 5\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# Transformar cada número del RDD: multiplicarlo por 2 y luego elevarlo al cuadrado\n",
        "rdd_transformado = rdd.map(lambda x: (x * 2) ** 2)\n",
        "\n",
        "# Recoger los resultados transformados en una lista para visualización\n",
        "resultado_transformado = rdd_transformado.collect()\n",
        "\n",
        "# Imprimir los números transformados\n",
        "print(resultado_transformado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHESz9f6ci2U",
        "outputId": "d981c36d-04c2-4812-db72-d655fb3f0690"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 16, 36, 64, 100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La diferencia principal entre `reduce` y `map` radica en **qué operación realizan y su propósito**:\n",
        "\n",
        "1. **`reduce`**:  \n",
        "   - Se usa para **reducir** todos los elementos de un RDD a un único valor combinándolos según una función.  \n",
        "   - En el ejemplo original, `reduce(lambda x, y: x + y)` toma dos números del RDD, los suma, y repite este proceso hasta que queda un solo valor, que es la suma total.  \n",
        "\n",
        "   ```python\n",
        "   resultado = rdd.reduce(lambda x, y: x + y)\n",
        "   # Suma todos los elementos del RDD.\n",
        "   ```\n",
        "\n",
        "2. **`map`**:  \n",
        "   - Se usa para **transformar** cada elemento del RDD aplicándole una función y devolviendo un nuevo RDD con los resultados.  \n",
        "   - En el código nuevo, `map(lambda x: (x * 2) ** 2)` toma cada número del RDD, lo multiplica por 2, lo eleva al cuadrado y genera un nuevo RDD con los valores transformados.  \n",
        "\n",
        "   ```python\n",
        "   rdd_transformado = rdd.map(lambda x: (x * 2) ** 2)\n",
        "   # Transforma cada elemento del RDD.\n",
        "   ```\n",
        "\n",
        "### Resumen:  \n",
        "- **`reduce`**: Reduce todo el conjunto a un único valor.  \n",
        "- **`map`**: Aplica una transformación a cada elemento, generando un nuevo conjunto con los valores transformados.  "
      ],
      "metadata": {
        "id": "_3SyMHL5c9_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "-JLxWGXJd5rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funcion `count`\n",
        "\n",
        "La función **`count`** en PySpark se usa para contar el número total de elementos en un RDD o DataFrame. Es útil para obtener el tamaño del conjunto de datos con el que estás trabajando."
      ],
      "metadata": {
        "id": "xGan6FVpeGC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con letras\n",
        "rdd_letras = sc.parallelize([\"a\", \"b\", \"c\", \"a\", \"b\", \"a\"])\n",
        "\n",
        "# Contar cuántos elementos hay en el RDD\n",
        "conteo_letras = rdd_letras.count()\n",
        "\n",
        "# Imprimir el resultado\n",
        "print(f\"El número total de letras es: {conteo_letras}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Iz7p2dzePFr",
        "outputId": "593bb724-1fa6-4b58-b2e9-58b277b2a7a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El número total de letras es: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con números utilizando range\n",
        "rdd_numeros = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Contar cuántos elementos hay en el RDD\n",
        "conteo_numeros = rdd_numeros.count()\n",
        "\n",
        "# Imprimir el resultado\n",
        "print(f\"El número total de números es: {conteo_numeros}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p1uyXv5fRf6",
        "outputId": "b65cccff-2359-4b97-be52-f89118faec8a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El número total de números es: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con valores booleanos\n",
        "rdd_booleanos = sc.parallelize([True, False, True, False, True])\n",
        "\n",
        "# Contar cuántos elementos hay en el RDD\n",
        "conteo_booleanos = rdd_booleanos.count()\n",
        "\n",
        "# Imprimir el resultado\n",
        "print(f\"El número total de valores booleanos es: {conteo_booleanos}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWA918pPfh-K",
        "outputId": "35f0971a-e751-494f-9841-174b862522d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El número total de valores booleanos es: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "k6uM6VM-fuZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funcion `collect`\n",
        "\n",
        "La función **`collect`** en PySpark se utiliza para recuperar todos los elementos de un RDD o DataFrame y traerlos al programa principal (driver) como una lista. Es útil para inspeccionar datos pequeños, pero debe usarse con cuidado en conjuntos de datos grandes para evitar problemas de memoria."
      ],
      "metadata": {
        "id": "24fgzQUffv66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con números\n",
        "rdd_numeros = sc.parallelize(range(1, 6))\n",
        "\n",
        "# Usar collect para traer los datos al programa principal\n",
        "resultado_numeros = rdd_numeros.collect()\n",
        "\n",
        "# Imprimir los números recogidos\n",
        "print(f\"Los números en el RDD son: {resultado_numeros}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNRvvH1kf0L6",
        "outputId": "850f3a31-9332-4a09-e740-c8a1f32c6277"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los números en el RDD son: [1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con cadenas de texto\n",
        "rdd_texto = sc.parallelize([\"hola\", \"mundo\", \"apache\", \"spark\"])\n",
        "\n",
        "# Usar collect para traer los datos al programa principal\n",
        "resultado_texto = rdd_texto.collect()\n",
        "\n",
        "# Imprimir las cadenas recogidas\n",
        "print(f\"Las cadenas en el RDD son: {resultado_texto}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5PyrtNQgFUq",
        "outputId": "5d587aa2-6fb7-4965-8e35-f1b5e2fd5110"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Las cadenas en el RDD son: ['hola', 'mundo', 'apache', 'spark']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con valores booleanos\n",
        "rdd_booleanos = sc.parallelize([True, False, True, True, False])\n",
        "\n",
        "# Usar collect para traer los datos al programa principal\n",
        "resultado_booleanos = rdd_booleanos.collect()\n",
        "\n",
        "# Imprimir los valores booleanos recogidos\n",
        "print(f\"Los valores booleanos en el RDD son: {resultado_booleanos}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIlex5izgIuK",
        "outputId": "62ba63a9-7b00-4a18-ca52-0468f94bbb3e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los valores booleanos en el RDD son: [True, False, True, True, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar números pares y multiplicarlos por 3\n",
        "# Crear un RDD con números del 1 al 20\n",
        "rdd_numeros = sc.parallelize(range(1, 21))\n",
        "\n",
        "# Filtrar los números pares\n",
        "rdd_pares = rdd_numeros.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Multiplicar los números pares por 3\n",
        "rdd_transformado = rdd_pares.map(lambda x: x * 3)\n",
        "\n",
        "# Usar collect para traer los resultados transformados al programa principal\n",
        "resultado = rdd_transformado.collect()\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f\"Los números pares multiplicados por 3 son: {resultado}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMTEU17WtRyD",
        "outputId": "44995b2a-a249-4679-a3b2-f35084f4b56d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los números pares multiplicados por 3 son: [6, 12, 18, 24, 30, 36, 42, 48, 54, 60]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar numero pares, elevarlos al cuadrado e imprimir tuplas (numero, numero**2)\n",
        "\n",
        "# Crear un RDD con números del 1 al 20\n",
        "rdd_numeros = sc.parallelize(range(1, 21))\n",
        "\n",
        "# Filtrar los números pares\n",
        "rdd_pares = rdd_numeros.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Crear tuplas (número, número**2)\n",
        "rdd_tuplas = rdd_pares.map(lambda x: (x, x ** 2))\n",
        "\n",
        "# Usar collect para traer los resultados transformados al programa principal\n",
        "resultado = rdd_tuplas.collect()\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f\"Las tuplas (número, número**2) son: {resultado}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcJE593quCRi",
        "outputId": "61c20fb2-c0f1-4e99-89e4-89b57c74e416"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Las tuplas (número, número**2) son: [(2, 4), (4, 16), (6, 36), (8, 64), (10, 100), (12, 144), (14, 196), (16, 256), (18, 324), (20, 400)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "-KykjHxz0Xy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Funciones take, max, y saveAsTextFile en PySpark**\n",
        "\n",
        "### Introducción\n",
        "En PySpark, existen funciones que permiten realizar operaciones específicas en los RDDs para obtener resultados o guardar datos. A continuación, se explican las funciones **`take`**, **`max`**, y **`saveAsTextFile`**, junto con ejemplos de uso y sus aplicaciones.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Función `take`\n",
        "\n",
        "#### Descripción:\n",
        "La función **`take`** se utiliza para recuperar un número específico de elementos del RDD, empezando desde el inicio. Es especialmente útil para inspeccionar los primeros elementos de un conjunto de datos sin necesidad de procesar todo el RDD.\n",
        "\n",
        "#### Sintaxis:\n",
        "```python\n",
        "rdd.take(n)\n",
        "```\n",
        "- **`n`**: El número de elementos que se desea recuperar.\n",
        "\n",
        "#### Ejemplo de uso:"
      ],
      "metadata": {
        "id": "3R9mg2Kz1qVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con números del 1 al 20\n",
        "rdd = sc.parallelize(range(1, 21))\n",
        "\n",
        "# Tomar los primeros 5 elementos\n",
        "resultado = rdd.take(5)\n",
        "\n",
        "# Imprimir el resultado\n",
        "print(f\"Los primeros 5 elementos son: {resultado}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyT6jdQp11a2",
        "outputId": "31018f0f-84e7-4ad8-d8cc-5e94cbaf8729"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los primeros 5 elementos son: [1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize('La programacion y la ciencia de datos son increibles. Me gusta todo esto!!'.split(' '))\n",
        "rdd.take(7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc__9qO52IUO",
        "outputId": "fd26a085-581c-4f01-9935-5a541d04435c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['La', 'programacion', 'y', 'la', 'ciencia', 'de', 'datos']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aplicaciones:\n",
        "- Inspeccionar una muestra inicial de datos.\n",
        "- Depuración rápida de código al verificar los elementos de un RDD."
      ],
      "metadata": {
        "id": "ZC_Jr0c41wl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "uQZQBTcR2pLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Función `max`\n",
        "\n",
        "#### Descripción:\n",
        "La función **`max`** se utiliza para encontrar el valor máximo en un RDD. Este método devuelve el elemento más grande según el orden natural de los datos o un criterio definido por el usuario.\n",
        "\n",
        "#### Sintaxis:\n",
        "```python\n",
        "rdd.max()\n",
        "```\n",
        "\n",
        "#### Ejemplo de uso:"
      ],
      "metadata": {
        "id": "oFq_NiSX2uKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con números del 1 al 10\n",
        "rdd = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Encontrar el valor máximo en el RDD\n",
        "maximo = rdd.max()\n",
        "\n",
        "# Imprimir el resultado\n",
        "print(f\"El valor máximo es: {maximo}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQDhIRM421vd",
        "outputId": "7037f84b-f96a-4358-f48e-907dbeb7a7a1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El valor máximo es: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aplicaciones:\n",
        "- Identificar el valor más alto en un conjunto de datos.\n",
        "- Encontrar el límite superior en métricas o medidas.\n"
      ],
      "metadata": {
        "id": "vKkRITSU22tV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "RYlO8uAR3aBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Función `saveAsTextFile`\n",
        "\n",
        "#### Descripción:\n",
        "\n",
        "La función **`saveAsTextFile`** se utiliza para guardar el contenido de un RDD en un archivo de texto. Cada elemento del RDD se escribe como una línea independiente en el archivo de salida. Esta función es útil para exportar los datos procesados a sistemas de archivos distribuidos como HDFS o al sistema local.\n",
        "\n",
        "#### Sintaxis:\n",
        "\n",
        "```python\n",
        "rdd.saveAsTextFile(path)\n",
        "```\n",
        "\n",
        "- **`path`**: La ruta donde se guardarán los datos.\n",
        "\n",
        "#### Ejemplo de uso:"
      ],
      "metadata": {
        "id": "DmxQJn4Q3dbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con una lista de cadenas\n",
        "rdd = sc.parallelize([\"PySpark\", \"es\", \"muy\", \"útil\"])\n",
        "\n",
        "# Guardar el RDD en un archivo de texto\n",
        "rdd.saveAsTextFile(\"salida_texto\")\n",
        "\n",
        "print(\"Datos guardados en la carpeta 'salida_texto'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xSXouFJ3lX1",
        "outputId": "ee341d8b-00f6-4ab4-8266-3f6fd46411c1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos guardados en la carpeta 'salida_texto'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Nota:** El archivo de salida será una carpeta que contendrá múltiples archivos de texto (partes) correspondientes a las particiones del RDD.\n",
        "\n",
        "#### Aplicaciones:\n",
        "\n",
        "- Exportar resultados procesados para su uso posterior.\n",
        "- Guardar datos en formato legible para su análisis externo.\n",
        "- Integrar datos procesados en PySpark con otros sistemas o herramientas."
      ],
      "metadata": {
        "id": "l7n5ElxF3lpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD con una lista de cadenas\n",
        "rdd = sc.parallelize([\"PySpark\", \"es\", \"muy\", \"útil\"])\n",
        "\n",
        "# Reducir el RDD a una sola partición\n",
        "rdd_unica_particion = rdd.coalesce(1)\n",
        "\n",
        "# Guardar el RDD en un archivo de texto con una sola partición\n",
        "rdd_unica_particion.saveAsTextFile(\"salida_unica_particion\")\n",
        "\n",
        "print(\"Datos guardados en la carpeta 'salida_unica_particion' en un solo archivo.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhRRfXH_4fld",
        "outputId": "17b8b488-bd3b-4d19-8b73-e08e5dedfd93"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos guardados en la carpeta 'salida_unica_particion' en un solo archivo.\n"
          ]
        }
      ]
    }
  ]
}