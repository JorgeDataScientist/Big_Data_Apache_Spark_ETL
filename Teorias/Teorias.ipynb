{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que es un RDD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un RDD (Resilient Distributed Dataset) es la unidad fundamental de datos en Apache Spark, y es una abstracción de datos distribuidos y tolerantes a fallos. Los RDDs son la estructura básica utilizada para representar y manipular datos en un entorno distribuido con Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí hay algunas características clave de los RDDs:\n",
    "\n",
    "Distribuido: Un RDD se divide en particiones que se distribuyen en diferentes nodos de un clúster de Spark. Esto permite que los datos se procesen de manera paralela en un entorno distribuido.\n",
    "\n",
    "Tolerante a fallos: Los RDDs mantienen la tolerancia a fallos a través del seguimiento de las transformaciones aplicadas a los datos. Si un nodo falla, Spark puede reconstruir las particiones perdidas a partir de la información sobre las transformaciones aplicadas.\n",
    "\n",
    "Inmutabilidad: Los RDDs son inmutables, lo que significa que una vez que se crea un RDD, no se puede modificar. Sin embargo, puedes realizar transformaciones para obtener nuevos RDDs basados en el original.\n",
    "\n",
    "Operaciones Transformacionales y de Acción: Puedes realizar dos tipos principales de operaciones en un RDD: transformaciones y acciones. Las transformaciones crean un nuevo RDD a partir de uno existente (por ejemplo, map, filter), mientras que las acciones realizan algún tipo de cómputo y devuelven un resultado al programa del usuario (por ejemplo, count, collect).\n",
    "\n",
    "Lazy Evaluation: Spark utiliza una estrategia llamada \"evaluación perezosa\" (lazy evaluation), lo que significa que las transformaciones en un RDD no se ejecutan de inmediato. Spark espera hasta que se solicita una acción para ejecutar las transformaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquí hay un ejemplo simple de cómo trabajar con RDDs en Spark usando PySpark (la interfaz de Spark para Python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola\n",
      "Mundo\n"
     ]
    }
   ],
   "source": [
    "# Importa la clase SparkContext desde la biblioteca PySpark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Intenta obtener o crear el SparkContext existente\n",
    "try:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "except:\n",
    "    # Si no existe, crea un nuevo SparkContext con modo local y nombre \"NuevaAplicacion\"\n",
    "    sc = SparkContext(\"local\", \"NuevaAplicacion\")\n",
    "\n",
    "try:\n",
    "    # Tu código Spark aquí\n",
    "\n",
    "    # Crea un RDD a partir de una lista de palabras (\"Hola\" y \"Mundo\")\n",
    "    rdd = sc.parallelize([\"Hola\", \"Mundo\"])\n",
    "\n",
    "    # Ejecuta una acción para recolectar los resultados del RDD en el programa principal\n",
    "    result = rdd.collect()\n",
    "\n",
    "    # Itera sobre los resultados e imprime cada palabra\n",
    "    for word in result:\n",
    "        print(word)\n",
    "\n",
    "finally:\n",
    "    # Detén el SparkContext al final, independientemente de si hay excepciones o no\n",
    "    sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
